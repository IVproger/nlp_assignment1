{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "### Student: Ivan Golov\n",
    "### Email: i.golov@innopolis.university\n",
    "### Group: AI-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Norvig's solution evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started working on the Assignment by evaluating the Norvig's solution and hightlighting the main drawbacks of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the train language corpus using nltk.corpus import gutenberg, reuters, brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/ivangolov/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/ivangolov/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/ivangolov/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('reuters')\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg, reuters, brown\n",
    "\n",
    "# Function to generate large corpus text\n",
    "def generate_large_corpus():\n",
    "    large_corpus_text = \"\"\n",
    "    # List of all Gutenberg file IDs\n",
    "    file_ids = gutenberg.fileids()\n",
    "\n",
    "    # Generate the large corpus by combining all texts\n",
    "    large_corpus_text = \"\\n\".join(gutenberg.raw(file_id) for file_id in file_ids)\n",
    "\n",
    "    # Add Reuters and Brown corpora to the large corpus\n",
    "    reuters_text = \" \".join(reuters.words())\n",
    "    brown_text = \" \".join(brown.words())\n",
    "\n",
    "    large_corpus_text += f\"\\n{reuters_text}\\n{brown_text}\"\n",
    "\n",
    "    return large_corpus_text\n",
    "\n",
    "# Save the large corpus to a text file\n",
    "def save_large_corpus(file_path, corpus_text):\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(corpus_text)\n",
    "\n",
    "# Generate and save the large corpus\n",
    "large_corpus_text = generate_large_corpus()\n",
    "save_large_corpus(\"data/train/language_corpus.txt\", large_corpus_text)\n",
    "print(\"Large corpus generated and saved to 'data/train/language_corpus.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Norvig model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NorvigSpellingCorrector_v1:\n",
    "    def __init__(self, corpus):\n",
    "        self.WORDS = Counter(self.words(corpus))\n",
    "        self.N = sum(self.WORDS.values())\n",
    "\n",
    "    def words(self, text):\n",
    "        return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "    def P(self, word):\n",
    "        \"Probability of `word`.\"\n",
    "        return self.WORDS[word] / self.N\n",
    "\n",
    "    def correction(self, word):\n",
    "        \"Most probable spelling correction for word.\"\n",
    "        return max(self.candidates(word), key=self.P)\n",
    "\n",
    "    def candidates(self, word):\n",
    "        \"Generate possible spelling corrections for word.\"\n",
    "        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])\n",
    "\n",
    "    def known(self, words):\n",
    "        \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "        return set(w for w in words if w in self.WORDS)\n",
    "\n",
    "    def edits1(self, word):\n",
    "        \"All edits that are one edit away from `word`.\"\n",
    "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(self, word):\n",
    "        \"All edits that are two edits away from `word`.\"\n",
    "        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NorvigSpellingCorrector_v1(large_corpus_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ivangolov/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = \"\"\n",
    "with open('data/test/test.txt') as f:\n",
    "    test_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the document into sentences\n",
    "sentences = sent_tokenize(test_data.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Initialize lists to store the data for the DataFrame\n",
    "original_sentences = []\n",
    "misspelled_sentences = []\n",
    "correct_words = []\n",
    "misspelled_words = []\n",
    "\n",
    "# Define a regex pattern to find the <ERR> tags\n",
    "pattern = re.compile(r'<err targ=(.*?)>(.*?)</err>')\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess_sentence(sentence):\n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # Remove punctuation\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    # Remove extra whitespaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "    return sentence\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    matches = pattern.findall(sentence)\n",
    "    if matches:\n",
    "        original_sentence = sentence\n",
    "        misspelled_sentence = sentence\n",
    "        correct_word_list = []\n",
    "        misspelled_word_list = []\n",
    "        \n",
    "        for match in matches:\n",
    "            correct_word = match[0]\n",
    "            misspelled_word = match[1]\n",
    "            misspelled_word_mew = match[1].replace(' ', '')\n",
    "            correct_word_list.append(correct_word)\n",
    "            misspelled_word_list.append(misspelled_word_mew)\n",
    "            misspelled_sentence = misspelled_sentence.replace(f'<err targ={correct_word}>{misspelled_word}</err>', misspelled_word_mew)\n",
    "            original_sentence = original_sentence.replace(f'<err targ={correct_word}>{misspelled_word}</err>', correct_word)\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        original_sentence = preprocess_sentence(original_sentence)\n",
    "        misspelled_sentence = preprocess_sentence(misspelled_sentence)\n",
    "        \n",
    "        original_sentences.append(original_sentence)\n",
    "        misspelled_sentences.append(misspelled_sentence)\n",
    "        correct_words.append(correct_word_list)\n",
    "        misspelled_words.append(misspelled_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Original Sentence  \\\n",
      "0  1 nigel thrush page 48 i have four in my famil...   \n",
      "1                          my sister goes to tonbury   \n",
      "2                          my mum goes out sometimes   \n",
      "3  i go to bridgebrook i go out sometimes on tues...   \n",
      "4  on thursday nights i go bellringing on saturda...   \n",
      "\n",
      "                                 Misspelled Sentence      Correct Words  \\\n",
      "0  1 nigel thrush page 48 i have four in my famil...           [sister]   \n",
      "1                             my siter go to tonbury     [sister, goes]   \n",
      "2                          my mum goes out sometimes        [sometimes]   \n",
      "3  i go to bridgebrook i go out sometimes on tues...  [sometimes, club]   \n",
      "4  on thursday nights i go bellringing on saturda...      [bellringing]   \n",
      "\n",
      "    Misspelled Words  \n",
      "0            [siter]  \n",
      "1        [siter, go]  \n",
      "2        [sometimes]  \n",
      "3  [sometimes, clob]  \n",
      "4      [bellringing]  \n"
     ]
    }
   ],
   "source": [
    "# Construct the pandas DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Original Sentence': original_sentences,\n",
    "    'Misspelled Sentence': misspelled_sentences,\n",
    "    'Correct Words': correct_words,\n",
    "    'Misspelled Words': misspelled_words\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('data/test/test_data_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Sentence</th>\n",
       "      <th>Misspelled Sentence</th>\n",
       "      <th>Correct Words</th>\n",
       "      <th>Misspelled Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 nigel thrush page 48 i have four in my famil...</td>\n",
       "      <td>1 nigel thrush page 48 i have four in my famil...</td>\n",
       "      <td>[sister]</td>\n",
       "      <td>[siter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my sister goes to tonbury</td>\n",
       "      <td>my siter go to tonbury</td>\n",
       "      <td>[sister, goes]</td>\n",
       "      <td>[siter, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my mum goes out sometimes</td>\n",
       "      <td>my mum goes out sometimes</td>\n",
       "      <td>[sometimes]</td>\n",
       "      <td>[sometimes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i go to bridgebrook i go out sometimes on tues...</td>\n",
       "      <td>i go to bridgebrook i go out sometimes on tues...</td>\n",
       "      <td>[sometimes, club]</td>\n",
       "      <td>[sometimes, clob]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on thursday nights i go bellringing on saturda...</td>\n",
       "      <td>on thursday nights i go bellringing on saturda...</td>\n",
       "      <td>[bellringing]</td>\n",
       "      <td>[bellringing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i go to bed at 10 o clock i watch tv at 5 o cl...</td>\n",
       "      <td>i go to bed at 10 o clock i wakh tv at 5 o clo...</td>\n",
       "      <td>[watch]</td>\n",
       "      <td>[wakh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the house is white it has stone up the front i...</td>\n",
       "      <td>the house is white it has stone up the frount ...</td>\n",
       "      <td>[front, second]</td>\n",
       "      <td>[frount, sexeon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>on monday i sometimes go down the farm in the ...</td>\n",
       "      <td>on monday i sometimes go down the farm in the ...</td>\n",
       "      <td>[watch]</td>\n",
       "      <td>[wach]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>we have got anglia like to watch cowboys</td>\n",
       "      <td>we have got anglia like to wach cowboys</td>\n",
       "      <td>[watch, cowboys]</td>\n",
       "      <td>[wach, cowboys]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>on tuesday i get off the bus and sometimes in ...</td>\n",
       "      <td>on tuesday i get off the bus and sometimes in ...</td>\n",
       "      <td>[sometimes, club]</td>\n",
       "      <td>[sometimes, colbe]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Original Sentence  \\\n",
       "0  1 nigel thrush page 48 i have four in my famil...   \n",
       "1                          my sister goes to tonbury   \n",
       "2                          my mum goes out sometimes   \n",
       "3  i go to bridgebrook i go out sometimes on tues...   \n",
       "4  on thursday nights i go bellringing on saturda...   \n",
       "5  i go to bed at 10 o clock i watch tv at 5 o cl...   \n",
       "6  the house is white it has stone up the front i...   \n",
       "7  on monday i sometimes go down the farm in the ...   \n",
       "8           we have got anglia like to watch cowboys   \n",
       "9  on tuesday i get off the bus and sometimes in ...   \n",
       "\n",
       "                                 Misspelled Sentence      Correct Words  \\\n",
       "0  1 nigel thrush page 48 i have four in my famil...           [sister]   \n",
       "1                             my siter go to tonbury     [sister, goes]   \n",
       "2                          my mum goes out sometimes        [sometimes]   \n",
       "3  i go to bridgebrook i go out sometimes on tues...  [sometimes, club]   \n",
       "4  on thursday nights i go bellringing on saturda...      [bellringing]   \n",
       "5  i go to bed at 10 o clock i wakh tv at 5 o clo...            [watch]   \n",
       "6  the house is white it has stone up the frount ...    [front, second]   \n",
       "7  on monday i sometimes go down the farm in the ...            [watch]   \n",
       "8            we have got anglia like to wach cowboys   [watch, cowboys]   \n",
       "9  on tuesday i get off the bus and sometimes in ...  [sometimes, club]   \n",
       "\n",
       "     Misspelled Words  \n",
       "0             [siter]  \n",
       "1         [siter, go]  \n",
       "2         [sometimes]  \n",
       "3   [sometimes, clob]  \n",
       "4       [bellringing]  \n",
       "5              [wakh]  \n",
       "6    [frount, sexeon]  \n",
       "7              [wach]  \n",
       "8     [wach, cowboys]  \n",
       "9  [sometimes, colbe]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk.metrics import edit_distance as Levenshtein\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import ast\n",
    "\n",
    "# Define the functions to calculate WER and CER\n",
    "def calculate_wer(reference, corrected):\n",
    "    # Calculate Word Error Rate (WER)\n",
    "    reference_words = reference.split()\n",
    "    corrected_words = corrected.split()\n",
    "\n",
    "    S = Levenshtein(reference_words, corrected_words)\n",
    "    I = max(0, len(corrected_words) - len(reference_words))\n",
    "    D = max(0, len(reference_words) - len(corrected_words))\n",
    "\n",
    "    N = max(len(reference_words), len(corrected_words))\n",
    "\n",
    "    wer = (S + I + D) / N\n",
    "\n",
    "    return wer\n",
    "\n",
    "def calculate_cer(reference, corrected):\n",
    "    # Calculate Character Error Rate (CER)\n",
    "    S = Levenshtein(reference, corrected)\n",
    "    I = max(0, len(corrected) - len(reference))\n",
    "    D = max(0, len(reference) - len(corrected))\n",
    "\n",
    "    N = max(len(reference), len(corrected))\n",
    "\n",
    "    cer = (S + I + D) / N\n",
    "\n",
    "    return cer\n",
    "\n",
    "# Define the function to calculate accuracy\n",
    "def calculate_accuracy(df):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        correct_words = ast.literal_eval(row['Correct Words'])\n",
    "        corrected_words = row['Corrected Words']\n",
    "        for correct_word, corrected_word in zip(correct_words, corrected_words):\n",
    "            if correct_word == corrected_word:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "def calculate_perplexity_norvig(sentence, model):\n",
    "    words = model.words(sentence)\n",
    "    log_prob = 0\n",
    "\n",
    "    for word in words:\n",
    "        prob = model.P(word)\n",
    "        if prob > 0:\n",
    "            log_prob += np.log2(prob)  \n",
    "        else:\n",
    "            log_prob += np.log2(1 / model.N) \n",
    "\n",
    "    HC = -log_prob / len(words)  # Cross-entropy\n",
    "    perpl = math.pow(2, HC)  # Perplexity\n",
    "\n",
    "    return HC, perpl\n",
    "\n",
    "def correct_sentence(sentence, model):\n",
    "    return ' '.join([model.correction(word) for word in model.words(sentence)])\n",
    "\n",
    "def correct_words(words, model):\n",
    "    return [model.correction(word.strip()) for word in eval(words)]\n",
    "\n",
    "def compute_stats(df, model, tag):\n",
    "    WER = []\n",
    "    CER = []\n",
    "    accuracy = 0\n",
    "    perplexities = []\n",
    "    HCs = []\n",
    "\n",
    "    if tag == \"Norvig\":\n",
    "        \n",
    "        print(\"Compute the corrected sentence and words\")\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            df['Corrected Sentence'] = list(tqdm(executor.map(lambda sentence: correct_sentence(sentence, model), df['Misspelled Sentence']), total=len(df)))\n",
    "            df['Corrected Words'] = list(tqdm(executor.map(lambda words: correct_words(words, model), df['Misspelled Words']), total=len(df)))\n",
    "        \n",
    "        print(\"Compute the WER, CER and Perplexity\")\n",
    "        for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            reference = row['Original Sentence']\n",
    "            corrected = row['Corrected Sentence']\n",
    "            wer = calculate_wer(reference, corrected)\n",
    "            cer = calculate_cer(reference, corrected)\n",
    "            WER.append(wer)\n",
    "            CER.append(cer)\n",
    "            HC, perplexity = calculate_perplexity_norvig(corrected, model)\n",
    "            perplexities.append(perplexity)\n",
    "            HCs.append(HC)\n",
    "            \n",
    "        \n",
    "        print(\"Compute the accuracy\")\n",
    "        accuracy = calculate_accuracy(df)\n",
    "            \n",
    "    print('Average Word Error Rate (WER):', np.mean(WER))\n",
    "    print('Average Character Error Rate (CER):', np.mean(CER))\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print('Average Perplexity:', np.mean(perplexities))\n",
    "    print('Average Cross-Entropy:', np.mean(HCs))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv('data/test/test_data_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the corrected sentence and words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666/666 [00:36<00:00, 18.09it/s] \n",
      "100%|██████████| 666/666 [00:23<00:00, 28.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the WER, CER and Perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666/666 [00:28<00:00, 22.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666/666 [00:00<00:00, 10956.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Word Error Rate (WER): 0.12568611368448648\n",
      "Average Character Error Rate (CER): 0.06691358406401805\n",
      "Accuracy: 0.2058484032320123\n",
      "Average Perplexity: 5198.027378935151\n",
      "Average Cross-Entropy: 10.281997326281457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "updated = compute_stats(test_df.copy(), model, \"Norvig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Sentence</th>\n",
       "      <th>Misspelled Sentence</th>\n",
       "      <th>Correct Words</th>\n",
       "      <th>Misspelled Words</th>\n",
       "      <th>Corrected Sentence</th>\n",
       "      <th>Corrected Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 nigel thrush page 48 i have four in my famil...</td>\n",
       "      <td>1 nigel thrush page 48 i have four in my famil...</td>\n",
       "      <td>['sister']</td>\n",
       "      <td>['siter']</td>\n",
       "      <td>1 nigel thrush page 48 i have four in my famil...</td>\n",
       "      <td>[sister]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my sister goes to tonbury</td>\n",
       "      <td>my siter go to tonbury</td>\n",
       "      <td>['sister', 'goes']</td>\n",
       "      <td>['siter', 'go']</td>\n",
       "      <td>my sister go to tilbury</td>\n",
       "      <td>[sister, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my mum goes out sometimes</td>\n",
       "      <td>my mum goes out sometimes</td>\n",
       "      <td>['sometimes']</td>\n",
       "      <td>['sometimes']</td>\n",
       "      <td>my mum goes out sometimes</td>\n",
       "      <td>[sometimes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i go to bridgebrook i go out sometimes on tues...</td>\n",
       "      <td>i go to bridgebrook i go out sometimes on tues...</td>\n",
       "      <td>['sometimes', 'club']</td>\n",
       "      <td>['sometimes', 'clob']</td>\n",
       "      <td>i go to bridgebrook i go out sometimes on tues...</td>\n",
       "      <td>[sometimes, club]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on thursday nights i go bellringing on saturda...</td>\n",
       "      <td>on thursday nights i go bellringing on saturda...</td>\n",
       "      <td>['bellringing']</td>\n",
       "      <td>['bellringing']</td>\n",
       "      <td>on thursday nights i go bellringing on saturda...</td>\n",
       "      <td>[bellringing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i go to bed at 10 o clock i watch tv at 5 o cl...</td>\n",
       "      <td>i go to bed at 10 o clock i wakh tv at 5 o clo...</td>\n",
       "      <td>['watch']</td>\n",
       "      <td>['wakh']</td>\n",
       "      <td>i go to bed at 10 o clock i wash tv at 5 o clo...</td>\n",
       "      <td>[wash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the house is white it has stone up the front i...</td>\n",
       "      <td>the house is white it has stone up the frount ...</td>\n",
       "      <td>['front', 'second']</td>\n",
       "      <td>['frount', 'sexeon']</td>\n",
       "      <td>the house is white it has stone up the front i...</td>\n",
       "      <td>[front, sexton]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>on monday i sometimes go down the farm in the ...</td>\n",
       "      <td>on monday i sometimes go down the farm in the ...</td>\n",
       "      <td>['watch']</td>\n",
       "      <td>['wach']</td>\n",
       "      <td>on monday i sometimes go down the farm in the ...</td>\n",
       "      <td>[each]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>we have got anglia like to watch cowboys</td>\n",
       "      <td>we have got anglia like to wach cowboys</td>\n",
       "      <td>['watch', 'cowboys']</td>\n",
       "      <td>['wach', 'cowboys']</td>\n",
       "      <td>we have got anglia like to each cowboys</td>\n",
       "      <td>[each, cowboys]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>on tuesday i get off the bus and sometimes in ...</td>\n",
       "      <td>on tuesday i get off the bus and sometimes in ...</td>\n",
       "      <td>['sometimes', 'club']</td>\n",
       "      <td>['sometimes', 'colbe']</td>\n",
       "      <td>on tuesday i get off the bus and sometimes in ...</td>\n",
       "      <td>[sometimes, cole]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Original Sentence  \\\n",
       "0  1 nigel thrush page 48 i have four in my famil...   \n",
       "1                          my sister goes to tonbury   \n",
       "2                          my mum goes out sometimes   \n",
       "3  i go to bridgebrook i go out sometimes on tues...   \n",
       "4  on thursday nights i go bellringing on saturda...   \n",
       "5  i go to bed at 10 o clock i watch tv at 5 o cl...   \n",
       "6  the house is white it has stone up the front i...   \n",
       "7  on monday i sometimes go down the farm in the ...   \n",
       "8           we have got anglia like to watch cowboys   \n",
       "9  on tuesday i get off the bus and sometimes in ...   \n",
       "\n",
       "                                 Misspelled Sentence          Correct Words  \\\n",
       "0  1 nigel thrush page 48 i have four in my famil...             ['sister']   \n",
       "1                             my siter go to tonbury     ['sister', 'goes']   \n",
       "2                          my mum goes out sometimes          ['sometimes']   \n",
       "3  i go to bridgebrook i go out sometimes on tues...  ['sometimes', 'club']   \n",
       "4  on thursday nights i go bellringing on saturda...        ['bellringing']   \n",
       "5  i go to bed at 10 o clock i wakh tv at 5 o clo...              ['watch']   \n",
       "6  the house is white it has stone up the frount ...    ['front', 'second']   \n",
       "7  on monday i sometimes go down the farm in the ...              ['watch']   \n",
       "8            we have got anglia like to wach cowboys   ['watch', 'cowboys']   \n",
       "9  on tuesday i get off the bus and sometimes in ...  ['sometimes', 'club']   \n",
       "\n",
       "         Misspelled Words                                 Corrected Sentence  \\\n",
       "0               ['siter']  1 nigel thrush page 48 i have four in my famil...   \n",
       "1         ['siter', 'go']                            my sister go to tilbury   \n",
       "2           ['sometimes']                          my mum goes out sometimes   \n",
       "3   ['sometimes', 'clob']  i go to bridgebrook i go out sometimes on tues...   \n",
       "4         ['bellringing']  on thursday nights i go bellringing on saturda...   \n",
       "5                ['wakh']  i go to bed at 10 o clock i wash tv at 5 o clo...   \n",
       "6    ['frount', 'sexeon']  the house is white it has stone up the front i...   \n",
       "7                ['wach']  on monday i sometimes go down the farm in the ...   \n",
       "8     ['wach', 'cowboys']            we have got anglia like to each cowboys   \n",
       "9  ['sometimes', 'colbe']  on tuesday i get off the bus and sometimes in ...   \n",
       "\n",
       "     Corrected Words  \n",
       "0           [sister]  \n",
       "1       [sister, go]  \n",
       "2        [sometimes]  \n",
       "3  [sometimes, club]  \n",
       "4      [bellringing]  \n",
       "5             [wash]  \n",
       "6    [front, sexton]  \n",
       "7             [each]  \n",
       "8    [each, cowboys]  \n",
       "9  [sometimes, cole]  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Norvig spelling corrector model v1\n",
    "* Average Word Error Rate (WER): 0.12577995752833032\n",
    "* Average Character Error Rate (CER): 0.06695512816430119\n",
    "* Accuracy: 0.2058484032320123\n",
    "* Average Perplexity: 5198.027378935151\n",
    "* Average Cross-Entropy: 10.281997326281457"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Improvement №1** (add advanced text predprocessing, exclude rare words and add <UNK> token):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NorvigSpellingCorrector_v2:\n",
    "    def __init__(self, corpus):\n",
    "        self.WORDS = self.preprocess_corpus(corpus)\n",
    "        self.N = sum(self.WORDS.values())\n",
    "        \n",
    "    def preprocess_corpus(self, corpus):\n",
    "        corpus = corpus.lower()\n",
    "\n",
    "        corpus = re.sub(r'[^a-z0-9\\s]', '', corpus)\n",
    "\n",
    "        tokens = self.words(corpus)  \n",
    "\n",
    "        word_counts = Counter(tokens)\n",
    "        \n",
    "        threshold = 10\n",
    "        vocab = {word for word, count in word_counts.items() if count >= threshold}\n",
    "\n",
    "        tokens = [token if token in vocab else \"<UNK>\" for token in tokens]\n",
    "\n",
    "        word_counts = Counter(tokens)\n",
    "    \n",
    "        return word_counts \n",
    "\n",
    "    def words(self, text):\n",
    "        return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "    def P(self, word):\n",
    "        \"Probability of `word`.\"\n",
    "        return self.WORDS[word] / self.N\n",
    "\n",
    "    def correction(self, word):\n",
    "        \"Most probable spelling correction for word.\"\n",
    "        return max(self.candidates(word), key=self.P)\n",
    "\n",
    "    def candidates(self, word):\n",
    "        \"Generate possible spelling corrections for word.\"\n",
    "        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])\n",
    "\n",
    "    def known(self, words):\n",
    "        \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "        return set(w for w in words if w in self.WORDS)\n",
    "\n",
    "    def edits1(self, word):\n",
    "        \"All edits that are one edit away from `word`.\"\n",
    "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(self, word):\n",
    "        \"All edits that are two edits away from `word`.\"\n",
    "        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_corpus_text = \"\"\n",
    "with open('data/train/language_corpus.txt') as f:\n",
    "    large_corpus_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NorvigSpellingCorrector_v2(large_corpus_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the corrected sentence and words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666/666 [00:52<00:00, 12.70it/s]\n",
      "100%|██████████| 666/666 [00:31<00:00, 21.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the WER, CER and Perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666/666 [00:29<00:00, 22.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666/666 [00:00<00:00, 9620.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Word Error Rate (WER): 0.12965260361783013\n",
      "Average Character Error Rate (CER): 0.06938579384714089\n",
      "Accuracy: 0.22008464794151597\n",
      "Average Perplexity: 4751.986770267538\n",
      "Average Cross-Entropy: 10.065830773255565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "updated = compute_stats(test_df.copy(), model, \"Norvig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Sentence</th>\n",
       "      <th>Misspelled Sentence</th>\n",
       "      <th>Correct Words</th>\n",
       "      <th>Misspelled Words</th>\n",
       "      <th>Corrected Sentence</th>\n",
       "      <th>Corrected Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 nigel thrush page 48 i have four in my famil...</td>\n",
       "      <td>1 nigel thrush page 48 i have four in my famil...</td>\n",
       "      <td>['sister']</td>\n",
       "      <td>['siter']</td>\n",
       "      <td>1 nigel thrust page 48 i have four in my famil...</td>\n",
       "      <td>[sister]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my sister goes to tonbury</td>\n",
       "      <td>my siter go to tonbury</td>\n",
       "      <td>['sister', 'goes']</td>\n",
       "      <td>['siter', 'go']</td>\n",
       "      <td>my sister go to tonbury</td>\n",
       "      <td>[sister, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my mum goes out sometimes</td>\n",
       "      <td>my mum goes out sometimes</td>\n",
       "      <td>['sometimes']</td>\n",
       "      <td>['sometimes']</td>\n",
       "      <td>my sum goes out sometimes</td>\n",
       "      <td>[sometimes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i go to bridgebrook i go out sometimes on tues...</td>\n",
       "      <td>i go to bridgebrook i go out sometimes on tues...</td>\n",
       "      <td>['sometimes', 'club']</td>\n",
       "      <td>['sometimes', 'clob']</td>\n",
       "      <td>i go to bridgebrook i go out sometimes on tues...</td>\n",
       "      <td>[sometimes, club]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on thursday nights i go bellringing on saturda...</td>\n",
       "      <td>on thursday nights i go bellringing on saturda...</td>\n",
       "      <td>['bellringing']</td>\n",
       "      <td>['bellringing']</td>\n",
       "      <td>on thursday nights i go bellringing on saturda...</td>\n",
       "      <td>[bellringing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i go to bed at 10 o clock i watch tv at 5 o cl...</td>\n",
       "      <td>i go to bed at 10 o clock i wakh tv at 5 o clo...</td>\n",
       "      <td>['watch']</td>\n",
       "      <td>['wakh']</td>\n",
       "      <td>i go to bed at 10 o clock i wash tv at 5 o clo...</td>\n",
       "      <td>[wash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the house is white it has stone up the front i...</td>\n",
       "      <td>the house is white it has stone up the frount ...</td>\n",
       "      <td>['front', 'second']</td>\n",
       "      <td>['frount', 'sexeon']</td>\n",
       "      <td>the house is white it has stone up the front i...</td>\n",
       "      <td>[front, seen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>on monday i sometimes go down the farm in the ...</td>\n",
       "      <td>on monday i sometimes go down the farm in the ...</td>\n",
       "      <td>['watch']</td>\n",
       "      <td>['wach']</td>\n",
       "      <td>on monday i sometimes go down the farm in the ...</td>\n",
       "      <td>[each]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>we have got anglia like to watch cowboys</td>\n",
       "      <td>we have got anglia like to wach cowboys</td>\n",
       "      <td>['watch', 'cowboys']</td>\n",
       "      <td>['wach', 'cowboys']</td>\n",
       "      <td>we have got angle like to each cowboy</td>\n",
       "      <td>[each, cowboy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>on tuesday i get off the bus and sometimes in ...</td>\n",
       "      <td>on tuesday i get off the bus and sometimes in ...</td>\n",
       "      <td>['sometimes', 'club']</td>\n",
       "      <td>['sometimes', 'colbe']</td>\n",
       "      <td>on tuesday i get off the bus and sometimes in ...</td>\n",
       "      <td>[sometimes, cole]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Original Sentence  \\\n",
       "0  1 nigel thrush page 48 i have four in my famil...   \n",
       "1                          my sister goes to tonbury   \n",
       "2                          my mum goes out sometimes   \n",
       "3  i go to bridgebrook i go out sometimes on tues...   \n",
       "4  on thursday nights i go bellringing on saturda...   \n",
       "5  i go to bed at 10 o clock i watch tv at 5 o cl...   \n",
       "6  the house is white it has stone up the front i...   \n",
       "7  on monday i sometimes go down the farm in the ...   \n",
       "8           we have got anglia like to watch cowboys   \n",
       "9  on tuesday i get off the bus and sometimes in ...   \n",
       "\n",
       "                                 Misspelled Sentence          Correct Words  \\\n",
       "0  1 nigel thrush page 48 i have four in my famil...             ['sister']   \n",
       "1                             my siter go to tonbury     ['sister', 'goes']   \n",
       "2                          my mum goes out sometimes          ['sometimes']   \n",
       "3  i go to bridgebrook i go out sometimes on tues...  ['sometimes', 'club']   \n",
       "4  on thursday nights i go bellringing on saturda...        ['bellringing']   \n",
       "5  i go to bed at 10 o clock i wakh tv at 5 o clo...              ['watch']   \n",
       "6  the house is white it has stone up the frount ...    ['front', 'second']   \n",
       "7  on monday i sometimes go down the farm in the ...              ['watch']   \n",
       "8            we have got anglia like to wach cowboys   ['watch', 'cowboys']   \n",
       "9  on tuesday i get off the bus and sometimes in ...  ['sometimes', 'club']   \n",
       "\n",
       "         Misspelled Words                                 Corrected Sentence  \\\n",
       "0               ['siter']  1 nigel thrust page 48 i have four in my famil...   \n",
       "1         ['siter', 'go']                            my sister go to tonbury   \n",
       "2           ['sometimes']                          my sum goes out sometimes   \n",
       "3   ['sometimes', 'clob']  i go to bridgebrook i go out sometimes on tues...   \n",
       "4         ['bellringing']  on thursday nights i go bellringing on saturda...   \n",
       "5                ['wakh']  i go to bed at 10 o clock i wash tv at 5 o clo...   \n",
       "6    ['frount', 'sexeon']  the house is white it has stone up the front i...   \n",
       "7                ['wach']  on monday i sometimes go down the farm in the ...   \n",
       "8     ['wach', 'cowboys']              we have got angle like to each cowboy   \n",
       "9  ['sometimes', 'colbe']  on tuesday i get off the bus and sometimes in ...   \n",
       "\n",
       "     Corrected Words  \n",
       "0           [sister]  \n",
       "1       [sister, go]  \n",
       "2        [sometimes]  \n",
       "3  [sometimes, club]  \n",
       "4      [bellringing]  \n",
       "5             [wash]  \n",
       "6      [front, seen]  \n",
       "7             [each]  \n",
       "8     [each, cowboy]  \n",
       "9  [sometimes, cole]  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Norvig spelling corrector model v2\n",
    "* Average Word Error Rate (WER): 0.12965260361783013\n",
    "* Average Character Error Rate (CER): 0.06938579384714089\n",
    "* Accuracy: 0.22008464794151597\n",
    "* Average Perplexity: 4751.986770267538\n",
    "* Average Cross-Entropy: 10.065830773255565"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Improvement №2** (Add the notion of context using N-gram models):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and predprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_corpus_text = \"\"\n",
    "with open('data/train/language_corpus.txt') as f:\n",
    "    large_corpus_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "        corpus = corpus.lower()\n",
    "\n",
    "        corpus = re.sub(r'[^a-z0-9\\s]', '', corpus)\n",
    "\n",
    "        tokens = re.findall(r'\\w+', corpus) \n",
    "\n",
    "        word_counts = Counter(tokens)\n",
    "        \n",
    "        threshold = 10\n",
    "        vocab = {word for word, count in word_counts.items() if count >= threshold}\n",
    "\n",
    "        tokens = [token if token in vocab else \"<UNK>\" for token in tokens]\n",
    "\n",
    "\n",
    "        return tokens, vocab\n",
    "\n",
    "corpus_words, vocab = preprocess_corpus(large_corpus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(sentences, vocab):\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in tqdm(sentences, total=len(sentences)):\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
    "        tokens = re.findall(r'\\w+', sentence)\n",
    "        preprocessed_sentence = [token if token in vocab else \"<UNK>\" for token in tokens]\n",
    "        preprocessed_sentences.append(preprocessed_sentence)\n",
    "    return preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sentences = nltk.sent_tokenize(large_corpus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248174/248174 [00:07<00:00, 33215.54it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_sentences_predprocessed = preprocess_sentences(corpus_sentences, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the N-gram stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building n-gram counts: 100%|██████████| 4624690/4624690 [00:14<00:00, 313352.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_ngram_counts(words, max_order):\n",
    "    \"\"\"\n",
    "    Build n-gram counts for orders 1 through max_order.\n",
    "    For unigrams, keys are one-element tuples.\n",
    "    \"\"\"\n",
    "    ngram_counts = {i: defaultdict(int) for i in range(1, max_order + 1)}\n",
    "\n",
    "    for i in tqdm(range(len(words)), desc=\"Building n-gram counts\"):\n",
    "        for order in range(1, max_order + 1):\n",
    "            if i + order <= len(words):\n",
    "                gram = tuple(words[i:i + order])\n",
    "                ngram_counts[order][gram] += 1\n",
    "    \n",
    "    return ngram_counts\n",
    "\n",
    "ngram_counts = build_ngram_counts(corpus_words, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram probability model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the bigram probabilities\n",
    "def compute_bigram_probabilities(w1, w2, ngram_counts, alpha):\n",
    "    \"\"\"\n",
    "    Computes smoothed bigram probability:\n",
    "      P(w2|w1) = (C(w1, w2) + alpha) / (C(w1) + alpha * |V|)\n",
    "    \"\"\"\n",
    "    bigram_count = ngram_counts[2][(w1, w2)]\n",
    "    unigram_count = ngram_counts[1][(w1,)]\n",
    "    \n",
    "    V = len(ngram_counts[1])\n",
    "    \n",
    "    bigram_probability = (bigram_count + alpha) / (unigram_count + alpha * V)\n",
    "    \n",
    "    return bigram_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_bigram(sentences, ngram_counts, alpha):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of the validation set using the bigram model.\n",
    "    \"\"\"\n",
    "    total_log_prob = 0\n",
    "    total_words = 0\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Computing Perplexity\"):\n",
    "        for i in range(1, len(sentence)):\n",
    "            w1 = sentence[i - 1]\n",
    "            w2 = sentence[i]\n",
    "            bigram_prob = compute_bigram_probabilities(w1, w2, ngram_counts, alpha)\n",
    "            log_prob = np.log2(bigram_prob)\n",
    "            total_log_prob += log_prob\n",
    "            total_words += 1\n",
    "\n",
    "    HC = -total_log_prob / total_words\n",
    "    perplexity = math.pow(2, HC)\n",
    "\n",
    "    return HC, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram probability model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the trigram probabilities\n",
    "def compute_trigram_probabilities(w1, w2, w3, ngram_counts, alpha):\n",
    "    \"\"\"\n",
    "    Computes smoothed trigram probability:\n",
    "      P(w3|w1, w2) = (C(w1, w2, w3) + alpha) / (C(w1, w2) + alpha * |V|)\n",
    "    \"\"\"\n",
    "    trigram_count = ngram_counts[3][(w1, w2, w3)]\n",
    "    bigram_count = ngram_counts[2][(w1, w2)]\n",
    "    \n",
    "    V = len(ngram_counts[1])\n",
    "    \n",
    "    trigram_probability = (trigram_count + alpha) / (bigram_count + alpha * V)\n",
    "    \n",
    "    return trigram_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_trigram(sentences, ngram_counts, alpha):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of the validation set using the trigram model.\n",
    "    \"\"\"\n",
    "    total_log_prob = 0\n",
    "    total_words = 0\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Computing Perplexity\"):\n",
    "        for i in range(2, len(sentence)):\n",
    "            w1 = sentence[i - 2]\n",
    "            w2 = sentence[i - 1]\n",
    "            w3 = sentence[i]\n",
    "            trigram_prob = compute_trigram_probabilities(w1, w2, w3, ngram_counts, alpha)\n",
    "            log_prob = np.log2(trigram_prob)\n",
    "            total_log_prob += log_prob\n",
    "            total_words += 1\n",
    "\n",
    "    HC = -total_log_prob / total_words\n",
    "    perplexity = math.pow(2, HC)\n",
    "\n",
    "    return HC, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolated bi-gram and tri-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_interpolated_prob(w1, w2, w3, ngram_counts, alpha, lamda):\n",
    "    \"\"\"\n",
    "    Computes the interpolated probability:\n",
    "      P(w3|w1,w2) = lam * P_trigram(w3|w1,w2) + (1 - lam) * P_bigram(w3|w2)\n",
    "    where the bigram probability is computed as:\n",
    "      P(w3|w2) = (C(w2, w3) + alpha) / (C(w2) + alpha * |V|)\n",
    "    \"\"\"\n",
    "    # Trigram probability\n",
    "    p_trigram = compute_trigram_probabilities(w1, w2, w3, ngram_counts, alpha)\n",
    "    \n",
    "    # Bigram probability\n",
    "    p_bigram = compute_bigram_probabilities(w2, w3, ngram_counts, alpha)\n",
    "    \n",
    "    # Interpolated probability\n",
    "    interpolated_prob = lamda * p_trigram + (1 - lamda) * p_bigram\n",
    "    \n",
    "    return interpolated_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_interpolated(sentences, ngram_counts, alpha, lamda):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of the validation set using the interpolated model.\n",
    "    \"\"\"\n",
    "    total_log_prob = 0\n",
    "    total_words = 0\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Computing Perplexity\"):\n",
    "        for i in range(2, len(sentence)):\n",
    "            w1 = sentence[i - 2]\n",
    "            w2 = sentence[i - 1]\n",
    "            w3 = sentence[i]\n",
    "            interpolated_prob = compute_interpolated_prob(w1, w2, w3, ngram_counts, alpha, lamda)\n",
    "            log_prob = np.log2(interpolated_prob)\n",
    "            total_log_prob += log_prob\n",
    "            total_words += 1\n",
    "\n",
    "    HC = -total_log_prob / total_words\n",
    "    perplexity = math.pow(2, HC)\n",
    "\n",
    "    return HC, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyperparameters for the bigram LM:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:01<00:00, 5044.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001, Cross-Entropy: 15.571766524590013, Perplexity: 48704.46861441161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:01<00:00, 5151.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Cross-Entropy: 14.330228241264058, Perplexity: 20598.16559884154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:01<00:00, 5164.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Cross-Entropy: 13.369777434082627, Perplexity: 10585.321255172768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:01<00:00, 5141.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, Cross-Entropy: 13.225124444839032, Perplexity: 9575.449149327107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4936.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.2, Cross-Entropy: 13.332784876705192, Perplexity: 10317.350239208281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:01<00:00, 5026.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.3, Cross-Entropy: 13.413407114210472, Perplexity: 10910.32996801804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:01<00:00, 5213.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5, Cross-Entropy: 13.525765811997951, Perplexity: 11794.002723707026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:01<00:00, 5114.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0, Cross-Entropy: 13.684862012267859, Perplexity: 13169.03500911066\n",
      "Tuning hyperparameters for the trigram LM:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4065.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001, Cross-Entropy: 14.620505569248602, Perplexity: 25188.98849920805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4396.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Cross-Entropy: 14.206140808074235, Perplexity: 18900.55277083627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4244.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Cross-Entropy: 14.159908223480418, Perplexity: 18304.46800107179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4483.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, Cross-Entropy: 14.215964158776348, Perplexity: 19029.686298875287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4257.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.2, Cross-Entropy: 14.236179841975247, Perplexity: 19298.215691654135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4436.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.3, Cross-Entropy: 14.247777104562108, Perplexity: 19453.971710877144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4391.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5, Cross-Entropy: 14.261719897904568, Perplexity: 19642.89427053955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4448.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0, Cross-Entropy: 14.278660100343172, Perplexity: 19874.90164292532\n",
      "Tuning hyperparameters for the interpolated LM:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3003.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001, Lambda: 0.1, Cross-Entropy: 13.74841975635381, Perplexity: 13762.16434002628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3085.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001, Lambda: 0.2, Cross-Entropy: 13.421845623129277, Perplexity: 10974.332889969628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3076.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001, Lambda: 0.3, Cross-Entropy: 13.251782886276027, Perplexity: 9754.031298146649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3071.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001, Lambda: 0.5, Cross-Entropy: 13.104681697956275, Perplexity: 8808.506410006923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3106.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001, Lambda: 0.7, Cross-Entropy: 13.126402758739374, Perplexity: 8942.129716459292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3051.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001, Lambda: 0.9, Cross-Entropy: 13.420482816570608, Perplexity: 10963.971149767824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2952.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Lambda: 0.1, Cross-Entropy: 13.388771662904029, Perplexity: 10725.606896728907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3092.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Lambda: 0.2, Cross-Entropy: 13.133015829305382, Perplexity: 8983.213017329448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3108.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Lambda: 0.3, Cross-Entropy: 12.990919827534746, Perplexity: 8140.602319926089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3092.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Lambda: 0.5, Cross-Entropy: 12.865048054124225, Perplexity: 7460.455769840212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3108.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Lambda: 0.7, Cross-Entropy: 12.889988452461294, Perplexity: 7590.5486469191455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2993.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Lambda: 0.9, Cross-Entropy: 13.170092411617906, Perplexity: 9217.069485687693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3098.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Lambda: 0.1, Cross-Entropy: 13.054219315058122, Perplexity: 8505.72982676796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2903.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Lambda: 0.2, Cross-Entropy: 12.95266768545199, Perplexity: 7927.595768650709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2895.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Lambda: 0.3, Cross-Entropy: 12.898792610216251, Perplexity: 7637.012184923165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3069.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Lambda: 0.5, Cross-Entropy: 12.874492605207696, Perplexity: 7509.455587400954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2707.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Lambda: 0.7, Cross-Entropy: 12.956232928853806, Perplexity: 7947.210974771716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3135.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Lambda: 0.9, Cross-Entropy: 13.264261707383362, Perplexity: 9838.766285362026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3074.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, Lambda: 0.1, Cross-Entropy: 13.171630429860954, Perplexity: 9226.900794201996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3035.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, Lambda: 0.2, Cross-Entropy: 13.165096248539044, Perplexity: 9185.205276259956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3094.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, Lambda: 0.3, Cross-Entropy: 13.176780061465879, Perplexity: 9259.894629423752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3134.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, Lambda: 0.5, Cross-Entropy: 13.243434492534057, Perplexity: 9697.750975674371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2921.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, Lambda: 0.7, Cross-Entropy: 13.381695482825537, Perplexity: 10673.128376878805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2810.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, Lambda: 0.9, Cross-Entropy: 13.688399665414488, Perplexity: 13201.366612424476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3093.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.2, Lambda: 0.1, Cross-Entropy: 13.313993041498236, Perplexity: 10183.832968208546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3032.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.2, Lambda: 0.2, Cross-Entropy: 13.322878081900287, Perplexity: 10246.745064096416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2722.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.2, Lambda: 0.3, Cross-Entropy: 13.344439653685741, Perplexity: 10401.036276166316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3096.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.2, Lambda: 0.5, Cross-Entropy: 13.422026880278523, Perplexity: 10975.711768527679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2755.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.2, Lambda: 0.7, Cross-Entropy: 13.560104693680929, Perplexity: 12078.089880640742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3124.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.2, Lambda: 0.9, Cross-Entropy: 13.836544304785765, Perplexity: 14629.007820328075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3059.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.3, Lambda: 0.1, Cross-Entropy: 13.408380719477208, Perplexity: 10872.38417705044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3082.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.3, Lambda: 0.2, Cross-Entropy: 13.423816716442913, Perplexity: 10989.336904925754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3075.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.3, Lambda: 0.3, Cross-Entropy: 13.449282648171627, Perplexity: 11185.03885910331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2978.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.3, Lambda: 0.5, Cross-Entropy: 13.529699897031351, Perplexity: 11826.207679021836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3116.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.3, Lambda: 0.7, Cross-Entropy: 13.66336565997831, Perplexity: 12974.269218489815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3077.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.3, Lambda: 0.9, Cross-Entropy: 13.915883410354882, Perplexity: 15456.04319086476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3131.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5, Lambda: 0.1, Cross-Entropy: 13.532514455118235, Perplexity: 11849.301983139223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3095.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5, Lambda: 0.2, Cross-Entropy: 13.553571800189744, Perplexity: 12023.520833735003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3046.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5, Lambda: 0.3, Cross-Entropy: 13.581963683365942, Perplexity: 12262.48442318979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3097.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5, Lambda: 0.5, Cross-Entropy: 13.662285258617828, Perplexity: 12964.55672183208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3107.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5, Lambda: 0.7, Cross-Entropy: 13.786314718599488, Perplexity: 14128.44157281574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3093.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5, Lambda: 0.9, Cross-Entropy: 14.004445186616572, Perplexity: 16434.559717363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:04<00:00, 2428.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0, Lambda: 0.1, Cross-Entropy: 13.699871746504796, Perplexity: 13306.760257644217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2692.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0, Lambda: 0.2, Cross-Entropy: 13.724104298991048, Perplexity: 13532.15792823409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3016.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0, Lambda: 0.3, Cross-Entropy: 13.753030375680103, Perplexity: 13806.216338878241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2884.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0, Lambda: 0.5, Cross-Entropy: 13.82730146463481, Perplexity: 14535.584492750848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2976.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0, Lambda: 0.7, Cross-Entropy: 13.932713511762609, Perplexity: 15637.40513465185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2822.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0, Lambda: 0.9, Cross-Entropy: 14.101701637133313, Perplexity: 17580.66031082511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tune the hyperparameters for the bigram LM\n",
    "validation_set = corpus_sentences[:10000]\n",
    "alpha_values = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.5, 1.0]\n",
    "lambda_values = [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "best_bigram_params=None\n",
    "best_bigram_ce = float('inf')\n",
    "print(\"Tuning hyperparameters for the bigram LM:\")\n",
    "for alpha in (alpha_values):\n",
    "    HC, perpl = compute_perplexity_bigram(validation_set, ngram_counts, alpha)\n",
    "    print(f\"Alpha: {alpha}, Cross-Entropy: {HC}, Perplexity: {perpl}\")\n",
    "    if HC < best_bigram_ce:\n",
    "        best_bigram_ce = HC\n",
    "        best_bigram_params = (alpha, HC, perpl)\n",
    "        \n",
    "# Tune the hyperparameters for the trigram LM\n",
    "best_trigram_params=None\n",
    "best_trigram_ce = float('inf')\n",
    "print(\"Tuning hyperparameters for the trigram LM:\")\n",
    "for alpha in (alpha_values):\n",
    "    HC, perpl = compute_perplexity_trigram(validation_set, ngram_counts, alpha)\n",
    "    print(f\"Alpha: {alpha}, Cross-Entropy: {HC}, Perplexity: {perpl}\")\n",
    "    if HC < best_trigram_ce:\n",
    "        best_trigram_ce = HC\n",
    "        best_trigram_params = (alpha, HC, perpl)\n",
    "        \n",
    "# Tune the hyperparameters for the interpolated LM\n",
    "best_interpolated_params=None\n",
    "best_interpolated_ce = float('inf')\n",
    "print(\"Tuning hyperparameters for the interpolated LM:\")\n",
    "for alpha in (alpha_values):\n",
    "    for lamda in lambda_values:\n",
    "        HC, perpl = compute_perplexity_interpolated(validation_set, ngram_counts, alpha, lamda)\n",
    "        print(f\"Alpha: {alpha}, Lambda: {lamda}, Cross-Entropy: {HC}, Perplexity: {perpl}\")\n",
    "        if HC < best_interpolated_ce:\n",
    "            best_interpolated_ce = HC\n",
    "            best_interpolated_params = (alpha, lamda, HC, perpl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for the bigram LM:\n",
      "Alpha: 0.1, Cross-Entropy: 13.225124444839032, Perplexity: 9575.449149327107\n",
      "Best hyperparameters for the trigram LM:\n",
      "Alpha: 0.01, Cross-Entropy: 14.159908223480418, Perplexity: 18304.46800107179\n",
      "Best hyperparameters for the interpolated LM:\n",
      "Alpha: 0.001, Lambda: 0.5, Cross-Entropy: 12.865048054124225, Perplexity: 7460.455769840212\n"
     ]
    }
   ],
   "source": [
    "# Print best paraneters\n",
    "print(\"Best hyperparameters for the bigram LM:\")\n",
    "print(f\"Alpha: {best_bigram_params[0]}, Cross-Entropy: {best_bigram_params[1]}, Perplexity: {best_bigram_params[2]}\")\n",
    "print(\"Best hyperparameters for the trigram LM:\")\n",
    "print(f\"Alpha: {best_trigram_params[0]}, Cross-Entropy: {best_trigram_params[1]}, Perplexity: {best_trigram_params[2]}\")\n",
    "print(\"Best hyperparameters for the interpolated LM:\")\n",
    "print(f\"Alpha: {best_interpolated_params[0]}, Lambda: {best_interpolated_params[1]}, Cross-Entropy: {best_interpolated_params[2]}, Perplexity: {best_interpolated_params[3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best hyperparameters\n",
    "best_hyperparameters = {\n",
    "    \"bigram\": best_bigram_params,\n",
    "    \"trigram\": best_trigram_params,\n",
    "    \"interpolated\": best_interpolated_params\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "with open('data/train/best_hyperparameters.json', 'w') as f:\n",
    "    json.dump(best_hyperparameters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution №2 N-gram model based on the Google Books n-gram API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "class ContextualSpellingCorrector:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = set(vocabulary)\n",
    "\n",
    "    def run_query(self, query, start_year=2010, end_year=2019, corpus=26, smoothing=3):\n",
    "        \"\"\"Fetches frequency data from the Google Books Ngram API.\"\"\"\n",
    "        query = urllib.parse.quote(query)\n",
    "        url = f'https://books.google.com/ngrams/json?content={query}&year_start={start_year}&year_end={end_year}&corpus={corpus}&smoothing={smoothing}'\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        try:\n",
    "            output = response.json()\n",
    "        except:\n",
    "            return {}\n",
    "\n",
    "        if not output:\n",
    "            return {}\n",
    "\n",
    "        return {entry['ngram']: sum(entry['timeseries']) / len(entry['timeseries']) for entry in output}\n",
    "\n",
    "    def average_frequency(self, phrase):\n",
    "        \"\"\"Gets the average frequency of a word or n-gram phrase from Google Ngrams.\"\"\"\n",
    "        data = self.run_query(phrase)\n",
    "        return sum(data.values()) / len(data) if data else 0\n",
    "\n",
    "    def words(self, text):\n",
    "        \"\"\"Tokenizes and lowercases the input text.\"\"\"\n",
    "        return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "    def correction_with_context(self, word, context_window):\n",
    "        \"\"\"Finds the best spelling correction by considering context-based n-gram probabilities.\"\"\"\n",
    "        candidates = self.candidates(word)\n",
    "        \n",
    "        # Generate n-grams with the surrounding context\n",
    "        context_phrases = {candidate: self.form_context_phrases(candidate, context_window) for candidate in candidates}\n",
    "\n",
    "        # Get frequencies for each candidate within its context\n",
    "        context_frequencies = {\n",
    "            candidate: sum(self.average_frequency(phrase) for phrase in phrases)\n",
    "            for candidate, phrases in context_phrases.items()\n",
    "        }\n",
    "\n",
    "        return max(context_frequencies, key=context_frequencies.get)  # Return the best correction\n",
    "\n",
    "    def form_context_phrases(self, candidate, context_window):\n",
    "        \"\"\"Forms bigram and trigram phrases including the candidate.\"\"\"\n",
    "        left_context, right_context = context_window\n",
    "        phrases = []\n",
    "\n",
    "        if left_context:\n",
    "            phrases.append(f\"{left_context} {candidate}\")\n",
    "        if right_context:\n",
    "            phrases.append(f\"{candidate} {right_context}\")\n",
    "        if left_context and right_context:\n",
    "            phrases.append(f\"{left_context} {candidate} {right_context}\")\n",
    "\n",
    "        return phrases\n",
    "\n",
    "    def candidates(self, word):\n",
    "        \"\"\"Generates possible spelling corrections based on known words.\"\"\"\n",
    "        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])\n",
    "\n",
    "    def known(self, words):\n",
    "        \"\"\"Filters words that exist in the vocabulary.\"\"\"\n",
    "        return set(w for w in words if w in self.vocabulary)\n",
    "\n",
    "    def edits1(self, word):\n",
    "        \"\"\"Generates possible single-edit variations of a word.\"\"\"\n",
    "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(self, word):\n",
    "        \"\"\"Generates possible double-edit variations of a word.\"\"\"\n",
    "        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n",
    "\n",
    "# Load corpus and build vocabulary\n",
    "with open(\"data/train/large_corpus.txt\") as f:\n",
    "    train_corpus = f.read()\n",
    "\n",
    "vocabulary = set(re.findall(r'\\w+', train_corpus.lower()))\n",
    "vocabulary.update([\"<START>\", \"<END>\"])  # Add special tokens\n",
    "\n",
    "# Initialize corrector with updated vocabulary\n",
    "corrector = ContextualSpellingCorrector(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize corrector\n",
    "corrector = ContextualSpellingCorrector(vocabulary)\n",
    "\n",
    "# Example usage with context-aware spelling correction\n",
    "sentence = [\"this\", \"is\", \"a\", \"speling\", \"error\"]\n",
    "corrected_list = []\n",
    "\n",
    "for i, word in enumerate(sentence):\n",
    "    left_context = sentence[i - 1] if i > 0 else \"<START>\"\n",
    "    right_context = sentence[i + 1] if i < len(sentence) - 1 else \"<END>\"\n",
    "    \n",
    "    corrected_word = corrector.correction_with_context(word, (left_context, right_context))\n",
    "    corrected_list.append(corrected_word)\n",
    "\n",
    "corrected_sentence = \" \".join(corrected_list)\n",
    "print(f\"Corrected sentence: {corrected_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each sentence\n",
    "corrected_sentences = []\n",
    "corrected_words_list = []\n",
    "\n",
    "for index, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "    misspelled_sentence = word_tokenize(row[\"Misspelled Sentence\"])  # Tokenize sentence\n",
    "    corrected_words = []  # Store corrected words\n",
    "    \n",
    "    corrected_sentence = []\n",
    "    for i, word in enumerate(misspelled_sentence):\n",
    "        left_context = misspelled_sentence[i - 1] if i > 0 else \"<START>\"\n",
    "        right_context = misspelled_sentence[i + 1] if i < len(misspelled_sentence) - 1 else \"<END>\"\n",
    "\n",
    "        corrected_word = corrector.correction_with_context(word, (left_context, right_context))\n",
    "        corrected_sentence.append(corrected_word)\n",
    "        \n",
    "        # If the word was corrected, add it to the corrected words list\n",
    "        if corrected_word != word:\n",
    "            corrected_words.append((word, corrected_word))\n",
    "\n",
    "    # Store results\n",
    "    corrected_sentences.append(\" \".join(corrected_sentence))\n",
    "    corrected_words_list.append(corrected_words)\n",
    "    break\n",
    "\n",
    "# Add results to DataFrame\n",
    "print(corrected_sentences)\n",
    "print(corrected_words_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
