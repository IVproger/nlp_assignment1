{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "### Student: Ivan Golov\n",
    "### Email: i.golov@innopolis.university\n",
    "### Group: AI-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **References:**\n",
    "\n",
    "1. **Norvig, P.**  \n",
    "   *How to Write a Spelling Corrector.*  \n",
    "   [https://norvig.com/spell-correct.html](https://norvig.com/spell-correct.html)\n",
    "\n",
    "2. **Voita, L.**  \n",
    "   *Theory of N-gram Models.*  \n",
    "   [https://lena-voita.github.io/nlp_course/language_modeling.html](https://lena-voita.github.io/nlp_course/language_modeling.html)\n",
    "\n",
    "3. **Corpora Provider**  \n",
    "   *Holbrook-tagged.dat Test Dataset Provider.*  \n",
    "   [https://titan.dcs.bbk.ac.uk/~roger/corpora.html](https://titan.dcs.bbk.ac.uk/~roger/corpora.html)\n",
    "\n",
    "4. **Deshmukh, D.**  \n",
    "   *Spelling Corrector Using N-gram Language Model (Kaggle).*  \n",
    "   [https://www.kaggle.com/code/dhruvdeshmukh/spelling-corrector-using-n-gram-language-model](https://www.kaggle.com/code/dhruvdeshmukh/spelling-corrector-using-n-gram-language-model)\n",
    "\n",
    "5. **Msamprovalaki**  \n",
    "   *Context-Aware Spelling Corrector - Assignment1 (GitHub).*  \n",
    "   [https://github.com/msamprovalaki/Context-Aware-Spelling-Corrector/blob/main/Assignment1.ipynb](https://github.com/msamprovalaki/Context-Aware-Spelling-Corrector/blob/main/Assignment1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Norvig's solution evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started working on the Assignment by evaluating the Norvig's solution and hightlighting the main drawbacks of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the train language corpus using nltk.corpus import gutenberg, reuters, brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/ivangolov/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/ivangolov/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/ivangolov/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large corpus generated and saved to 'data/train/language_corpus.txt'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('reuters')\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg, reuters, brown\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to generate large corpus text\n",
    "def generate_large_corpus():\n",
    "    large_corpus_text = \"\"\n",
    "\n",
    "    # List of all Gutenberg file IDs\n",
    "    file_ids = gutenberg.fileids()\n",
    "\n",
    "    # Generate the large corpus by combining all texts\n",
    "    large_corpus_text = \"\\n\".join(gutenberg.raw(file_id) for file_id in file_ids)\n",
    "\n",
    "    # Add Reuters and Brown corpora to the large corpus\n",
    "    reuters_text = \" \".join(reuters.words())\n",
    "    brown_text = \" \".join(brown.words())\n",
    "\n",
    "    large_corpus_text += f\"\\n{reuters_text}\\n{brown_text}\"\n",
    "\n",
    "    return large_corpus_text\n",
    "\n",
    "# Save the large corpus to a text file\n",
    "def save_large_corpus(file_path, corpus_text):\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(corpus_text)\n",
    "\n",
    "# Generate and save the large corpus\n",
    "large_corpus_text = generate_large_corpus()\n",
    "save_large_corpus(\"data/train/language_corpus.txt\", large_corpus_text)\n",
    "print(\"Large corpus generated and saved to 'data/train/language_corpus.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even at the very beginning of the paper, the author points out that he trained his model on a limited data set. Therefore, I decided to immediately collect a more diverse corpus from different texts using the nltk library. I guess it should hel me to:\n",
    "1) **Obtain broader vocabulary & diversity**\n",
    "2) **Build based on big corpus the improved N-gram Estimation (sense of context)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Norvig model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NorvigSpellingCorrector_v1:\n",
    "    def __init__(self, corpus):\n",
    "        self.WORDS = Counter(self.words(corpus))\n",
    "        self.N = sum(self.WORDS.values())\n",
    "\n",
    "    def words(self, text):\n",
    "        return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "    def P(self, word):\n",
    "        \"Probability of `word`.\"\n",
    "        return self.WORDS[word] / self.N\n",
    "\n",
    "    def correction(self, word):\n",
    "        \"Most probable spelling correction for word.\"\n",
    "        return max(self.candidates(word), key=self.P)\n",
    "\n",
    "    def candidates(self, word):\n",
    "        \"Generate possible spelling corrections for word.\"\n",
    "        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])\n",
    "\n",
    "    def known(self, words):\n",
    "        \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "        return set(w for w in words if w in self.WORDS)\n",
    "\n",
    "    def edits1(self, word):\n",
    "        \"All edits that are one edit away from `word`.\"\n",
    "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(self, word):\n",
    "        \"All edits that are two edits away from `word`.\"\n",
    "        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NorvigSpellingCorrector_v1(large_corpus_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ivangolov/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = \"\"\n",
    "with open('data/test/test.txt') as f:\n",
    "    test_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the document into sentences\n",
    "sentences = sent_tokenize(test_data.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Initialize lists to store the data for the DataFrame\n",
    "original_sentences = []\n",
    "misspelled_sentences = []\n",
    "correct_words = []\n",
    "misspelled_words = []\n",
    "\n",
    "# Define a regex pattern to find the <ERR> tags\n",
    "pattern = re.compile(r'<err targ=(.*?)>(.*?)</err>')\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess_sentence(sentence):\n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # Remove punctuation\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    # Remove extra whitespaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "    return sentence\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    matches = pattern.findall(sentence)\n",
    "    if matches:\n",
    "        original_sentence = sentence\n",
    "        misspelled_sentence = sentence\n",
    "        correct_word_list = []\n",
    "        misspelled_word_list = []\n",
    "        \n",
    "        for match in matches:\n",
    "            correct_word = match[0]\n",
    "            misspelled_word = match[1]\n",
    "            misspelled_word_mew = match[1].replace(' ', '')\n",
    "            correct_word_list.append(correct_word)\n",
    "            misspelled_word_list.append(misspelled_word_mew)\n",
    "            misspelled_sentence = misspelled_sentence.replace(f'<err targ={correct_word}>{misspelled_word}</err>', misspelled_word_mew)\n",
    "            original_sentence = original_sentence.replace(f'<err targ={correct_word}>{misspelled_word}</err>', correct_word)\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        original_sentence = preprocess_sentence(original_sentence)\n",
    "        misspelled_sentence = preprocess_sentence(misspelled_sentence)\n",
    "        \n",
    "        original_sentences.append(original_sentence)\n",
    "        misspelled_sentences.append(misspelled_sentence)\n",
    "        correct_words.append(correct_word_list)\n",
    "        misspelled_words.append(misspelled_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Original Sentence  \\\n",
      "0  1 nigel thrush page 48 i have four in my famil...   \n",
      "1                          my sister goes to tonbury   \n",
      "2                          my mum goes out sometimes   \n",
      "3  i go to bridgebrook i go out sometimes on tues...   \n",
      "4  on thursday nights i go bellringing on saturda...   \n",
      "\n",
      "                                 Misspelled Sentence      Correct Words  \\\n",
      "0  1 nigel thrush page 48 i have four in my famil...           [sister]   \n",
      "1                             my siter go to tonbury     [sister, goes]   \n",
      "2                          my mum goes out sometimes        [sometimes]   \n",
      "3  i go to bridgebrook i go out sometimes on tues...  [sometimes, club]   \n",
      "4  on thursday nights i go bellringing on saturda...      [bellringing]   \n",
      "\n",
      "    Misspelled Words  \n",
      "0            [siter]  \n",
      "1        [siter, go]  \n",
      "2        [sometimes]  \n",
      "3  [sometimes, clob]  \n",
      "4      [bellringing]  \n"
     ]
    }
   ],
   "source": [
    "# Construct the pandas DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Original Sentence': original_sentences,\n",
    "    'Misspelled Sentence': misspelled_sentences,\n",
    "    'Correct Words': correct_words,\n",
    "    'Misspelled Words': misspelled_words\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('data/test/test_data_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Sentence</th>\n",
       "      <th>Misspelled Sentence</th>\n",
       "      <th>Correct Words</th>\n",
       "      <th>Misspelled Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 nigel thrush page 48 i have four in my famil...</td>\n",
       "      <td>1 nigel thrush page 48 i have four in my famil...</td>\n",
       "      <td>[sister]</td>\n",
       "      <td>[siter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my sister goes to tonbury</td>\n",
       "      <td>my siter go to tonbury</td>\n",
       "      <td>[sister, goes]</td>\n",
       "      <td>[siter, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my mum goes out sometimes</td>\n",
       "      <td>my mum goes out sometimes</td>\n",
       "      <td>[sometimes]</td>\n",
       "      <td>[sometimes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i go to bridgebrook i go out sometimes on tues...</td>\n",
       "      <td>i go to bridgebrook i go out sometimes on tues...</td>\n",
       "      <td>[sometimes, club]</td>\n",
       "      <td>[sometimes, clob]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on thursday nights i go bellringing on saturda...</td>\n",
       "      <td>on thursday nights i go bellringing on saturda...</td>\n",
       "      <td>[bellringing]</td>\n",
       "      <td>[bellringing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i go to bed at 10 o clock i watch tv at 5 o cl...</td>\n",
       "      <td>i go to bed at 10 o clock i wakh tv at 5 o clo...</td>\n",
       "      <td>[watch]</td>\n",
       "      <td>[wakh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the house is white it has stone up the front i...</td>\n",
       "      <td>the house is white it has stone up the frount ...</td>\n",
       "      <td>[front, second]</td>\n",
       "      <td>[frount, sexeon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>on monday i sometimes go down the farm in the ...</td>\n",
       "      <td>on monday i sometimes go down the farm in the ...</td>\n",
       "      <td>[watch]</td>\n",
       "      <td>[wach]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>we have got anglia like to watch cowboys</td>\n",
       "      <td>we have got anglia like to wach cowboys</td>\n",
       "      <td>[watch, cowboys]</td>\n",
       "      <td>[wach, cowboys]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>on tuesday i get off the bus and sometimes in ...</td>\n",
       "      <td>on tuesday i get off the bus and sometimes in ...</td>\n",
       "      <td>[sometimes, club]</td>\n",
       "      <td>[sometimes, colbe]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Original Sentence  \\\n",
       "0  1 nigel thrush page 48 i have four in my famil...   \n",
       "1                          my sister goes to tonbury   \n",
       "2                          my mum goes out sometimes   \n",
       "3  i go to bridgebrook i go out sometimes on tues...   \n",
       "4  on thursday nights i go bellringing on saturda...   \n",
       "5  i go to bed at 10 o clock i watch tv at 5 o cl...   \n",
       "6  the house is white it has stone up the front i...   \n",
       "7  on monday i sometimes go down the farm in the ...   \n",
       "8           we have got anglia like to watch cowboys   \n",
       "9  on tuesday i get off the bus and sometimes in ...   \n",
       "\n",
       "                                 Misspelled Sentence      Correct Words  \\\n",
       "0  1 nigel thrush page 48 i have four in my famil...           [sister]   \n",
       "1                             my siter go to tonbury     [sister, goes]   \n",
       "2                          my mum goes out sometimes        [sometimes]   \n",
       "3  i go to bridgebrook i go out sometimes on tues...  [sometimes, club]   \n",
       "4  on thursday nights i go bellringing on saturda...      [bellringing]   \n",
       "5  i go to bed at 10 o clock i wakh tv at 5 o clo...            [watch]   \n",
       "6  the house is white it has stone up the frount ...    [front, second]   \n",
       "7  on monday i sometimes go down the farm in the ...            [watch]   \n",
       "8            we have got anglia like to wach cowboys   [watch, cowboys]   \n",
       "9  on tuesday i get off the bus and sometimes in ...  [sometimes, club]   \n",
       "\n",
       "     Misspelled Words  \n",
       "0             [siter]  \n",
       "1         [siter, go]  \n",
       "2         [sometimes]  \n",
       "3   [sometimes, clob]  \n",
       "4       [bellringing]  \n",
       "5              [wakh]  \n",
       "6    [frount, sexeon]  \n",
       "7              [wach]  \n",
       "8     [wach, cowboys]  \n",
       "9  [sometimes, colbe]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to evaluating accuracy and perplexity, I also incorporated Word Error Rate (WER) and Character Error Rate (CER) into the evaluation process. These metrics help measure how closely the model's corrected output aligns with the reference text, ensuring that the system corrects only genuine misspellings while preserving correctly spelled words. This comprehensive evaluation approach allows us to better understand the model's performance and its fidelity to the original content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Word Error Rate (WER)\n",
    "\n",
    "WER is a metric used to evaluate the performance of systems that generate or correct sequences of words, such as automatic speech recognition systems or spelling correction systems. It measures the number of word-level errors made by a system when comparing its output to the reference or ground truth text.\n",
    "\n",
    "### $ WER = \\frac{S + I + D}{N} $\n",
    "\n",
    "* $ S $ : the number of substitutions\n",
    "* $ i $ : the number of insertions\n",
    "* $ D $ : the number of deletions\n",
    "* N: the total number of words in the reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Error Rate (CER)\n",
    "CER is similar to WER but operates at the character level. It measures the accuracy of character-level transcriptions or corrections. CER quantifies the number of character-level errors made by a system when comparing its output to the reference text. Errors include substitutions, insertions, and deletions of individual characters.\n",
    "\n",
    "### $ WER = \\frac{S + I + D}{N} $\n",
    "\n",
    "* $ S $ : the number of character substitutions\n",
    "* $ i $ : the number of character insertions\n",
    "* $ D $ : the number of character deletions\n",
    "* N: the total number of characterσ in the reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk.metrics import edit_distance as Levenshtein\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import ast\n",
    "\n",
    "# Define the functions to calculate WER and CER\n",
    "def calculate_wer(reference, corrected):\n",
    "    # Calculate Word Error Rate (WER)\n",
    "    reference_words = reference.split()\n",
    "    corrected_words = corrected.split()\n",
    "\n",
    "    S = Levenshtein(reference_words, corrected_words)\n",
    "    I = max(0, len(corrected_words) - len(reference_words))\n",
    "    D = max(0, len(reference_words) - len(corrected_words))\n",
    "\n",
    "    N = max(len(reference_words), len(corrected_words))\n",
    "\n",
    "    wer = (S + I + D) / N\n",
    "\n",
    "    return wer\n",
    "\n",
    "def calculate_cer(reference, corrected):\n",
    "    # Calculate Character Error Rate (CER)\n",
    "    S = Levenshtein(reference, corrected)\n",
    "    I = max(0, len(corrected) - len(reference))\n",
    "    D = max(0, len(reference) - len(corrected))\n",
    "\n",
    "    N = max(len(reference), len(corrected))\n",
    "\n",
    "    cer = (S + I + D) / N\n",
    "\n",
    "    return cer\n",
    "\n",
    "# Define the function to calculate accuracy\n",
    "def calculate_accuracy(df):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        correct_words = ast.literal_eval(row['Correct Words'])\n",
    "        corrected_words = row['Corrected Words']\n",
    "        for correct_word, corrected_word in zip(correct_words, corrected_words):\n",
    "            if correct_word == corrected_word:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "def calculate_perplexity_norvig(sentence, model):\n",
    "    words = model.words(sentence)\n",
    "    log_prob = 0\n",
    "\n",
    "    for word in words:\n",
    "        prob = model.P(word)\n",
    "        if prob > 0:\n",
    "            log_prob += np.log2(prob)  \n",
    "        else:\n",
    "            log_prob += np.log2(1 / model.N) \n",
    "\n",
    "    HC = -log_prob / len(words)  # Cross-entropy\n",
    "    perpl = math.pow(2, HC)  # Perplexity\n",
    "\n",
    "    return HC, perpl\n",
    "\n",
    "def correct_sentence(sentence, model):\n",
    "    return ' '.join([model.correction(word) for word in model.words(sentence)])\n",
    "\n",
    "def correct_words(words, model):\n",
    "    return [model.correction(word.strip()) for word in eval(words)]\n",
    "\n",
    "def compute_stats(df, model, tag):\n",
    "    WER = []\n",
    "    CER = []\n",
    "    accuracy = 0\n",
    "    perplexities = []\n",
    "    HCs = []\n",
    "\n",
    "    if tag == \"Norvig\":\n",
    "        \n",
    "        print(\"Compute the corrected sentence and words\")\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            df['Corrected Sentence'] = list(tqdm(executor.map(lambda sentence: correct_sentence(sentence, model), df['Misspelled Sentence']), total=len(df)))\n",
    "            df['Corrected Words'] = list(tqdm(executor.map(lambda words: correct_words(words, model), df['Misspelled Words']), total=len(df)))\n",
    "        \n",
    "        print(\"Compute the WER, CER and Perplexity\")\n",
    "        for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            reference = row['Original Sentence']\n",
    "            corrected = row['Corrected Sentence']\n",
    "            wer = calculate_wer(reference, corrected)\n",
    "            cer = calculate_cer(reference, corrected)\n",
    "            WER.append(wer)\n",
    "            CER.append(cer)\n",
    "            HC, perplexity = calculate_perplexity_norvig(corrected, model)\n",
    "            perplexities.append(perplexity)\n",
    "            HCs.append(HC)\n",
    "            \n",
    "        \n",
    "        print(\"Compute the accuracy\")\n",
    "        accuracy = calculate_accuracy(df)\n",
    "            \n",
    "    print('Average Word Error Rate (WER):', np.mean(WER))\n",
    "    print('Average Character Error Rate (CER):', np.mean(CER))\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print('Average Perplexity:', np.mean(perplexities))\n",
    "    print('Average Cross-Entropy:', np.mean(HCs))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv('data/test/test_data_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the corrected sentence and words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666/666 [00:32<00:00, 20.44it/s] \n",
      "100%|██████████| 666/666 [00:30<00:00, 21.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the WER, CER and Perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666/666 [00:28<00:00, 23.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666/666 [00:00<00:00, 12108.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Word Error Rate (WER): 0.12587380137217416\n",
      "Average Character Error Rate (CER): 0.0669916914630596\n",
      "Accuracy: 0.2054636398614852\n",
      "Average Perplexity: 5198.027378935151\n",
      "Average Cross-Entropy: 10.281997326281457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "updated = compute_stats(test_df.copy(), model, \"Norvig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Sentence</th>\n",
       "      <th>Misspelled Sentence</th>\n",
       "      <th>Correct Words</th>\n",
       "      <th>Misspelled Words</th>\n",
       "      <th>Corrected Sentence</th>\n",
       "      <th>Corrected Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 nigel thrush page 48 i have four in my famil...</td>\n",
       "      <td>1 nigel thrush page 48 i have four in my famil...</td>\n",
       "      <td>['sister']</td>\n",
       "      <td>['siter']</td>\n",
       "      <td>1 nigel thrush page 48 i have four in my famil...</td>\n",
       "      <td>[sister]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my sister goes to tonbury</td>\n",
       "      <td>my siter go to tonbury</td>\n",
       "      <td>['sister', 'goes']</td>\n",
       "      <td>['siter', 'go']</td>\n",
       "      <td>my sister go to tilbury</td>\n",
       "      <td>[sister, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my mum goes out sometimes</td>\n",
       "      <td>my mum goes out sometimes</td>\n",
       "      <td>['sometimes']</td>\n",
       "      <td>['sometimes']</td>\n",
       "      <td>my mum goes out sometimes</td>\n",
       "      <td>[sometimes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i go to bridgebrook i go out sometimes on tues...</td>\n",
       "      <td>i go to bridgebrook i go out sometimes on tues...</td>\n",
       "      <td>['sometimes', 'club']</td>\n",
       "      <td>['sometimes', 'clob']</td>\n",
       "      <td>i go to bridgebrook i go out sometimes on tues...</td>\n",
       "      <td>[sometimes, club]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on thursday nights i go bellringing on saturda...</td>\n",
       "      <td>on thursday nights i go bellringing on saturda...</td>\n",
       "      <td>['bellringing']</td>\n",
       "      <td>['bellringing']</td>\n",
       "      <td>on thursday nights i go bellringing on saturda...</td>\n",
       "      <td>[bellringing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i go to bed at 10 o clock i watch tv at 5 o cl...</td>\n",
       "      <td>i go to bed at 10 o clock i wakh tv at 5 o clo...</td>\n",
       "      <td>['watch']</td>\n",
       "      <td>['wakh']</td>\n",
       "      <td>i go to bed at 10 o clock i wash tv at 5 o clo...</td>\n",
       "      <td>[wash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the house is white it has stone up the front i...</td>\n",
       "      <td>the house is white it has stone up the frount ...</td>\n",
       "      <td>['front', 'second']</td>\n",
       "      <td>['frount', 'sexeon']</td>\n",
       "      <td>the house is white it has stone up the front i...</td>\n",
       "      <td>[front, sexton]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>on monday i sometimes go down the farm in the ...</td>\n",
       "      <td>on monday i sometimes go down the farm in the ...</td>\n",
       "      <td>['watch']</td>\n",
       "      <td>['wach']</td>\n",
       "      <td>on monday i sometimes go down the farm in the ...</td>\n",
       "      <td>[each]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>we have got anglia like to watch cowboys</td>\n",
       "      <td>we have got anglia like to wach cowboys</td>\n",
       "      <td>['watch', 'cowboys']</td>\n",
       "      <td>['wach', 'cowboys']</td>\n",
       "      <td>we have got anglia like to each cowboys</td>\n",
       "      <td>[each, cowboys]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>on tuesday i get off the bus and sometimes in ...</td>\n",
       "      <td>on tuesday i get off the bus and sometimes in ...</td>\n",
       "      <td>['sometimes', 'club']</td>\n",
       "      <td>['sometimes', 'colbe']</td>\n",
       "      <td>on tuesday i get off the bus and sometimes in ...</td>\n",
       "      <td>[sometimes, cole]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Original Sentence  \\\n",
       "0  1 nigel thrush page 48 i have four in my famil...   \n",
       "1                          my sister goes to tonbury   \n",
       "2                          my mum goes out sometimes   \n",
       "3  i go to bridgebrook i go out sometimes on tues...   \n",
       "4  on thursday nights i go bellringing on saturda...   \n",
       "5  i go to bed at 10 o clock i watch tv at 5 o cl...   \n",
       "6  the house is white it has stone up the front i...   \n",
       "7  on monday i sometimes go down the farm in the ...   \n",
       "8           we have got anglia like to watch cowboys   \n",
       "9  on tuesday i get off the bus and sometimes in ...   \n",
       "\n",
       "                                 Misspelled Sentence          Correct Words  \\\n",
       "0  1 nigel thrush page 48 i have four in my famil...             ['sister']   \n",
       "1                             my siter go to tonbury     ['sister', 'goes']   \n",
       "2                          my mum goes out sometimes          ['sometimes']   \n",
       "3  i go to bridgebrook i go out sometimes on tues...  ['sometimes', 'club']   \n",
       "4  on thursday nights i go bellringing on saturda...        ['bellringing']   \n",
       "5  i go to bed at 10 o clock i wakh tv at 5 o clo...              ['watch']   \n",
       "6  the house is white it has stone up the frount ...    ['front', 'second']   \n",
       "7  on monday i sometimes go down the farm in the ...              ['watch']   \n",
       "8            we have got anglia like to wach cowboys   ['watch', 'cowboys']   \n",
       "9  on tuesday i get off the bus and sometimes in ...  ['sometimes', 'club']   \n",
       "\n",
       "         Misspelled Words                                 Corrected Sentence  \\\n",
       "0               ['siter']  1 nigel thrush page 48 i have four in my famil...   \n",
       "1         ['siter', 'go']                            my sister go to tilbury   \n",
       "2           ['sometimes']                          my mum goes out sometimes   \n",
       "3   ['sometimes', 'clob']  i go to bridgebrook i go out sometimes on tues...   \n",
       "4         ['bellringing']  on thursday nights i go bellringing on saturda...   \n",
       "5                ['wakh']  i go to bed at 10 o clock i wash tv at 5 o clo...   \n",
       "6    ['frount', 'sexeon']  the house is white it has stone up the front i...   \n",
       "7                ['wach']  on monday i sometimes go down the farm in the ...   \n",
       "8     ['wach', 'cowboys']            we have got anglia like to each cowboys   \n",
       "9  ['sometimes', 'colbe']  on tuesday i get off the bus and sometimes in ...   \n",
       "\n",
       "     Corrected Words  \n",
       "0           [sister]  \n",
       "1       [sister, go]  \n",
       "2        [sometimes]  \n",
       "3  [sometimes, club]  \n",
       "4      [bellringing]  \n",
       "5             [wash]  \n",
       "6    [front, sexton]  \n",
       "7             [each]  \n",
       "8    [each, cowboys]  \n",
       "9  [sometimes, cole]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Norvig spelling corrector model (Baseline):**\n",
    "\n",
    "* Average Word Error Rate (WER): 0.12587380137217416\n",
    "* Average Character Error Rate (CER): 0.0669916914630596\n",
    "* Accuracy: 0.2054636398614852\n",
    "* Average Perplexity: 5198.027378935151\n",
    "* Average Cross-Entropy: 10.281997326281457"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Planned Future Improvements:**\n",
    "\n",
    "1. **Enhanced Data Preprocessing:**  \n",
    "   Refining the preprocessing steps—such as advanced tokenization, normalization, and noise reduction—will help create a cleaner dataset, which in turn can improve the reliability of the n-gram statistics.\n",
    "\n",
    "2. **Contextual Modeling with N-grams:**  \n",
    "   Moving beyond a simple unigram approach, incorporating bigrams, trigrams, and interpolated models will allow the corrector to better understand and leverage contextual information. This is crucial for making distinctions in ambiguous cases, such as \"doing sport\" versus \"dying species.\"\n",
    "\n",
    "3. **Beam Search Implementation:**  \n",
    "   Integrating a beam search algorithm will enable more efficient exploration of correction candidates. This strategy can significantly enhance performance by balancing the search space with the quality of candidate sequences.\n",
    "\n",
    "4. **Alpha-Lambda Parameter Tuning:**  \n",
    "   Fine-tuning the smoothing parameters (alpha and lambda) in the interpolated models is expected to yield more accurate probability estimations. This tuning will help optimize the balance between n-gram levels, reducing both WER and CER while lowering perplexity and cross-entropy.\n",
    "\n",
    "Overall, these improvements aim to create a more robust, context-sensitive spelling corrector that better aligns with the source text and minimizes unnecessary changes to correctly spelled words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Improvement №1** (add advanced text predprocessing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NorvigSpellingCorrector_v2:\n",
    "    def __init__(self, corpus):\n",
    "        self.WORDS = self.preprocess_corpus(corpus)\n",
    "        self.N = sum(self.WORDS.values())\n",
    "    \n",
    "    # UPDATED: Preprocess the corpus to remove rare words and replace them with \"<UNK>\"\n",
    "    def preprocess_corpus(self, corpus):\n",
    "        # Convert the entire corpus to lowercase\n",
    "        corpus = corpus.lower()\n",
    "\n",
    "        # Remove all characters that are not letters, digits, or whitespace\n",
    "        corpus = re.sub(r'[^a-z0-9\\s]', '', corpus)\n",
    "\n",
    "        # Tokenize the corpus into words\n",
    "        tokens = self.words(corpus)  \n",
    "\n",
    "        # Count the frequency of each word in the corpus\n",
    "        word_counts = Counter(tokens)\n",
    "    \n",
    "        # Define a threshold for word frequency\n",
    "        threshold = 10\n",
    "        \n",
    "        # Create a vocabulary set with words that appear at least 'threshold' times\n",
    "        vocab = {word for word, count in word_counts.items() if count >= threshold}\n",
    "\n",
    "        # Replace words not in the vocabulary with the \"<UNK>\" token\n",
    "        tokens = [token if token in vocab else \"<UNK>\" for token in tokens]\n",
    "\n",
    "        # Recount the frequency of each word after replacing rare words with \"<UNK>\"\n",
    "        word_counts = Counter(tokens)\n",
    "        \n",
    "        # Return the final word counts\n",
    "        return word_counts\n",
    "\n",
    "    def words(self, text):\n",
    "        return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "    def P(self, word):\n",
    "        \"Probability of `word`.\"\n",
    "        return self.WORDS[word] / self.N\n",
    "\n",
    "    def correction(self, word):\n",
    "        \"Most probable spelling correction for word.\"\n",
    "        return max(self.candidates(word), key=self.P)\n",
    "\n",
    "    def candidates(self, word):\n",
    "        \"Generate possible spelling corrections for word.\"\n",
    "        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])\n",
    "\n",
    "    def known(self, words):\n",
    "        \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "        return set(w for w in words if w in self.WORDS)\n",
    "\n",
    "    def edits1(self, word):\n",
    "        \"All edits that are one edit away from `word`.\"\n",
    "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(self, word):\n",
    "        \"All edits that are two edits away from `word`.\"\n",
    "        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_corpus_text = \"\"\n",
    "with open('data/train/language_corpus.txt') as f:\n",
    "    large_corpus_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NorvigSpellingCorrector_v2(large_corpus_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the corrected sentence and words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666/666 [00:51<00:00, 13.03it/s] \n",
      "100%|██████████| 666/666 [00:28<00:00, 23.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the WER, CER and Perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666/666 [00:29<00:00, 22.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666/666 [00:00<00:00, 8219.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Word Error Rate (WER): 0.12967006293761502\n",
      "Average Character Error Rate (CER): 0.06937547142922464\n",
      "Accuracy: 0.21969988457098885\n",
      "Average Perplexity: 4751.986770267538\n",
      "Average Cross-Entropy: 10.065830773255565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "updated = compute_stats(test_df.copy(), model, \"Norvig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Sentence</th>\n",
       "      <th>Misspelled Sentence</th>\n",
       "      <th>Correct Words</th>\n",
       "      <th>Misspelled Words</th>\n",
       "      <th>Corrected Sentence</th>\n",
       "      <th>Corrected Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 nigel thrush page 48 i have four in my famil...</td>\n",
       "      <td>1 nigel thrush page 48 i have four in my famil...</td>\n",
       "      <td>['sister']</td>\n",
       "      <td>['siter']</td>\n",
       "      <td>1 nigel thrust page 48 i have four in my famil...</td>\n",
       "      <td>[sister]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my sister goes to tonbury</td>\n",
       "      <td>my siter go to tonbury</td>\n",
       "      <td>['sister', 'goes']</td>\n",
       "      <td>['siter', 'go']</td>\n",
       "      <td>my sister go to tonbury</td>\n",
       "      <td>[sister, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my mum goes out sometimes</td>\n",
       "      <td>my mum goes out sometimes</td>\n",
       "      <td>['sometimes']</td>\n",
       "      <td>['sometimes']</td>\n",
       "      <td>my sum goes out sometimes</td>\n",
       "      <td>[sometimes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i go to bridgebrook i go out sometimes on tues...</td>\n",
       "      <td>i go to bridgebrook i go out sometimes on tues...</td>\n",
       "      <td>['sometimes', 'club']</td>\n",
       "      <td>['sometimes', 'clob']</td>\n",
       "      <td>i go to bridgebrook i go out sometimes on tues...</td>\n",
       "      <td>[sometimes, club]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on thursday nights i go bellringing on saturda...</td>\n",
       "      <td>on thursday nights i go bellringing on saturda...</td>\n",
       "      <td>['bellringing']</td>\n",
       "      <td>['bellringing']</td>\n",
       "      <td>on thursday nights i go bellringing on saturda...</td>\n",
       "      <td>[bellringing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i go to bed at 10 o clock i watch tv at 5 o cl...</td>\n",
       "      <td>i go to bed at 10 o clock i wakh tv at 5 o clo...</td>\n",
       "      <td>['watch']</td>\n",
       "      <td>['wakh']</td>\n",
       "      <td>i go to bed at 10 o clock i wash tv at 5 o clo...</td>\n",
       "      <td>[wash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the house is white it has stone up the front i...</td>\n",
       "      <td>the house is white it has stone up the frount ...</td>\n",
       "      <td>['front', 'second']</td>\n",
       "      <td>['frount', 'sexeon']</td>\n",
       "      <td>the house is white it has stone up the front i...</td>\n",
       "      <td>[front, seen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>on monday i sometimes go down the farm in the ...</td>\n",
       "      <td>on monday i sometimes go down the farm in the ...</td>\n",
       "      <td>['watch']</td>\n",
       "      <td>['wach']</td>\n",
       "      <td>on monday i sometimes go down the farm in the ...</td>\n",
       "      <td>[each]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>we have got anglia like to watch cowboys</td>\n",
       "      <td>we have got anglia like to wach cowboys</td>\n",
       "      <td>['watch', 'cowboys']</td>\n",
       "      <td>['wach', 'cowboys']</td>\n",
       "      <td>we have got angle like to each cowboy</td>\n",
       "      <td>[each, cowboy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>on tuesday i get off the bus and sometimes in ...</td>\n",
       "      <td>on tuesday i get off the bus and sometimes in ...</td>\n",
       "      <td>['sometimes', 'club']</td>\n",
       "      <td>['sometimes', 'colbe']</td>\n",
       "      <td>on tuesday i get off the bus and sometimes in ...</td>\n",
       "      <td>[sometimes, cole]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Original Sentence  \\\n",
       "0  1 nigel thrush page 48 i have four in my famil...   \n",
       "1                          my sister goes to tonbury   \n",
       "2                          my mum goes out sometimes   \n",
       "3  i go to bridgebrook i go out sometimes on tues...   \n",
       "4  on thursday nights i go bellringing on saturda...   \n",
       "5  i go to bed at 10 o clock i watch tv at 5 o cl...   \n",
       "6  the house is white it has stone up the front i...   \n",
       "7  on monday i sometimes go down the farm in the ...   \n",
       "8           we have got anglia like to watch cowboys   \n",
       "9  on tuesday i get off the bus and sometimes in ...   \n",
       "\n",
       "                                 Misspelled Sentence          Correct Words  \\\n",
       "0  1 nigel thrush page 48 i have four in my famil...             ['sister']   \n",
       "1                             my siter go to tonbury     ['sister', 'goes']   \n",
       "2                          my mum goes out sometimes          ['sometimes']   \n",
       "3  i go to bridgebrook i go out sometimes on tues...  ['sometimes', 'club']   \n",
       "4  on thursday nights i go bellringing on saturda...        ['bellringing']   \n",
       "5  i go to bed at 10 o clock i wakh tv at 5 o clo...              ['watch']   \n",
       "6  the house is white it has stone up the frount ...    ['front', 'second']   \n",
       "7  on monday i sometimes go down the farm in the ...              ['watch']   \n",
       "8            we have got anglia like to wach cowboys   ['watch', 'cowboys']   \n",
       "9  on tuesday i get off the bus and sometimes in ...  ['sometimes', 'club']   \n",
       "\n",
       "         Misspelled Words                                 Corrected Sentence  \\\n",
       "0               ['siter']  1 nigel thrust page 48 i have four in my famil...   \n",
       "1         ['siter', 'go']                            my sister go to tonbury   \n",
       "2           ['sometimes']                          my sum goes out sometimes   \n",
       "3   ['sometimes', 'clob']  i go to bridgebrook i go out sometimes on tues...   \n",
       "4         ['bellringing']  on thursday nights i go bellringing on saturda...   \n",
       "5                ['wakh']  i go to bed at 10 o clock i wash tv at 5 o clo...   \n",
       "6    ['frount', 'sexeon']  the house is white it has stone up the front i...   \n",
       "7                ['wach']  on monday i sometimes go down the farm in the ...   \n",
       "8     ['wach', 'cowboys']              we have got angle like to each cowboy   \n",
       "9  ['sometimes', 'colbe']  on tuesday i get off the bus and sometimes in ...   \n",
       "\n",
       "     Corrected Words  \n",
       "0           [sister]  \n",
       "1       [sister, go]  \n",
       "2        [sometimes]  \n",
       "3  [sometimes, club]  \n",
       "4      [bellringing]  \n",
       "5             [wash]  \n",
       "6      [front, seen]  \n",
       "7             [each]  \n",
       "8     [each, cowboy]  \n",
       "9  [sometimes, cole]  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Key Aspects of the `preprocess_corpus` Function:**\n",
    "\n",
    "- **Lowercasing:**  \n",
    "  Converting the entire corpus to lowercase to ensure consistency.\n",
    "\n",
    "- **Noise Reduction:**  \n",
    "  Removing all characters except letters, digits, and whitespace using regular expressions.\n",
    "\n",
    "- **Tokenization:**  \n",
    "  Splitting the text into tokens to facilitate word-level analysis.\n",
    "\n",
    "- **Frequency Filtering:**  \n",
    "  Counting word occurrences and defining a threshold (set to 10) to build a robust vocabulary.\n",
    "\n",
    "- **Handling Rare Words:**  \n",
    "  Replacing words that do not meet the frequency threshold with an `<UNK>` token, thereby reducing the impact of rare or noisy tokens on the model.\n",
    "\n",
    "---\n",
    "\n",
    "**After Incorporating the Preprocessing Enhancements, the Updated Model Produced the Following Metrics:**\n",
    "\n",
    "### The Norvig Spelling Corrector Model (Baseline + Corpus Preprocessing)\n",
    "- **Average Word Error Rate (WER):** 0.12967006293761502  \n",
    "- **Average Character Error Rate (CER):** 0.06937547142922464  \n",
    "- **Accuracy:** 0.21969988457098885  \n",
    "- **Average Perplexity:** 4751.986770267538  \n",
    "- **Average Cross-Entropy:** 10.065830773255565  \n",
    "\n",
    "---\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- **Accuracy:**  \n",
    "  Increased from 0.20546 to 0.21970, indicating that the model now correctly handles a higher proportion of words.\n",
    "\n",
    "- **Perplexity & Cross-Entropy:**  \n",
    "  Both metrics decreased, suggesting that the language model has become more confident and is assigning higher probabilities to the correct word sequences.\n",
    "\n",
    "- **WER & CER:**  \n",
    "  Slight increases in error rates suggest that while the vocabulary standardization and unknown token replacement helped overall performance, they may have introduced a few extra misclassifications—likely due to aggressive filtering of infrequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Improvement №2** (Add the notion of context using N-gram models):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and predprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_corpus_text = \"\"\n",
    "with open('data/train/language_corpus.txt') as f:\n",
    "    large_corpus_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "        corpus = corpus.lower()\n",
    "\n",
    "        corpus = re.sub(r'[^a-z0-9\\s]', '', corpus)\n",
    "\n",
    "        tokens = re.findall(r'\\w+', corpus) \n",
    "        \n",
    "        # exclude single character words\n",
    "        tokens = [token for token in tokens if len(token) > 1]\n",
    "\n",
    "        word_counts = Counter(tokens)\n",
    "        \n",
    "        threshold = 10\n",
    "        vocab = {word for word, count in word_counts.items() if count >= threshold}\n",
    "\n",
    "        return tokens, vocab\n",
    "\n",
    "corpus_words, vocab = preprocess_corpus(large_corpus_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ I should note that since we do not have the ability to collect and curate a highly diverse text corpus, the introduction of the <UNK> token might be neglected. In our current setup, the probability of encountering infrequent words is already low, meaning that most words not meeting the frequency threshold would naturally be treated as rare. As a result, the impact of explicitly replacing them with <UNK> may not be significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(sentences):\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in tqdm(sentences, total=len(sentences)):\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
    "        tokens = re.findall(r'\\w+', sentence)\n",
    "        preprocessed_sentences.append(tokens)\n",
    "    return preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sentences = nltk.sent_tokenize(large_corpus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248174/248174 [00:04<00:00, 50922.44it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_sentences_predprocessed = preprocess_sentences(corpus_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the N-gram stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building n-gram counts: 100%|██████████| 4427046/4427046 [00:13<00:00, 327640.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_ngram_counts(words, max_order):\n",
    "    \"\"\"\n",
    "    Build n-gram counts for orders 1 through max_order.\n",
    "    For unigrams, keys are one-element tuples.\n",
    "    \"\"\"\n",
    "    ngram_counts = {i: defaultdict(int) for i in range(1, max_order + 1)}\n",
    "\n",
    "    for i in tqdm(range(len(words)), desc=\"Building n-gram counts\"):\n",
    "        for order in range(1, max_order + 1):\n",
    "            if i + order <= len(words):\n",
    "                gram = tuple(words[i:i + order])\n",
    "                ngram_counts[order][gram] += 1\n",
    "    \n",
    "    return ngram_counts\n",
    "\n",
    "ngram_counts = build_ngram_counts(corpus_words, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that our current use of a defaultdict efficiently manages n-gram storage without demanding excessive time or memory resources. However, as our corpus scales, there are several strategies we could adopt to optimize storage further. For instance, using a trie (prefix tree) can facilitate quick retrieval and efficient storage of n-grams, while database solutions like SQLite or PostgreSQL—with proper indexing—can handle larger datasets. Additionally, compressed storage methods (e.g., gzip) and sparse matrix representations (using libraries like scipy.sparse) can reduce disk space usage and memory consumption for very large or sparse n-gram datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the n-gram counts with gzip\n",
    "def save_ngram_counts(ngram_counts, filename):\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        pickle.dump(ngram_counts, f)\n",
    "\n",
    "# Save the n-gram counts to a file\n",
    "save_ngram_counts(ngram_counts, 'data/ngrams/ngram_counts.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the n-gram counts from a file\n",
    "def load_ngram_counts(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Example of loading the n-gram counts\n",
    "loaded_ngram_counts = load_ngram_counts('data/ngrams/ngram_counts.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram probability model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unigram_probability(candidate, ngram_counts, alpha, total_words, V):\n",
    "    \"\"\"\n",
    "    Computes smoothed unigram probability:\n",
    "      P(candidate) = (C(candidate) + alpha) / (total_words + alpha * |V|)\n",
    "    \"\"\"\n",
    "    unigram_count = ngram_counts[1].get((candidate,), 0)\n",
    "    V_size = len(V)\n",
    "    \n",
    "    unigram_probability = (unigram_count + alpha) / (total_words + alpha * V_size)\n",
    "    \n",
    "    return unigram_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram probability model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the bigram probabilities\n",
    "def compute_bigram_probabilities(w1, w2, ngram_counts, alpha):\n",
    "    \"\"\"\n",
    "    Computes smoothed bigram probability:\n",
    "      P(w2|w1) = (C(w1, w2) + alpha) / (C(w1) + alpha * |V|)\n",
    "    \"\"\"\n",
    "    bigram_count = ngram_counts[2][(w1, w2)]\n",
    "    unigram_count = ngram_counts[1][(w1,)]\n",
    "    \n",
    "    V = len(ngram_counts[1])\n",
    "    \n",
    "    bigram_probability = (bigram_count + alpha) / (unigram_count + alpha * V)\n",
    "    \n",
    "    return bigram_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_bigram(sentences, ngram_counts, alpha):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of the validation set using the bigram model.\n",
    "    \"\"\"\n",
    "    total_log_prob = 0\n",
    "    total_words = 0\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Computing Perplexity\"):\n",
    "        for i in range(1, len(sentence)):\n",
    "            w1 = sentence[i - 1]\n",
    "            w2 = sentence[i]\n",
    "            bigram_prob = compute_bigram_probabilities(w1, w2, ngram_counts, alpha)\n",
    "            log_prob = np.log2(bigram_prob)\n",
    "            total_log_prob += log_prob\n",
    "            total_words += 1\n",
    "\n",
    "    HC = -total_log_prob / total_words\n",
    "    perplexity = math.pow(2, HC)\n",
    "\n",
    "    return HC, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_bigram_avg(sentences, ngram_counts, alpha):\n",
    "    \"\"\"\n",
    "    Compute the average perplexity of the validation set using the bigram model.\n",
    "    \"\"\"\n",
    "    total_perplexity = 0\n",
    "    num_sentences = len(sentences)\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Computing Perplexity\"):\n",
    "        total_log_prob = 0\n",
    "        total_words = 0\n",
    "        for i in range(1, len(sentence)):\n",
    "            w1 = sentence[i - 1]\n",
    "            w2 = sentence[i]\n",
    "            bigram_prob = compute_bigram_probabilities(w1, w2, ngram_counts, alpha)\n",
    "            log_prob = np.log2(bigram_prob)\n",
    "            total_log_prob += log_prob\n",
    "            total_words += 1\n",
    "\n",
    "        HC = -total_log_prob / total_words\n",
    "        sentence_perplexity = math.pow(2, HC)\n",
    "        total_perplexity += sentence_perplexity\n",
    "\n",
    "    average_perplexity = total_perplexity / num_sentences\n",
    "\n",
    "    return HC, average_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Corpus-Level vs. Sentence-Level Calculation:**  \n",
    "  - **`compute_perplexity_bigram`:**  \n",
    "    This function aggregates the log probabilities for all bigrams across the entire corpus. It computes a single cross-entropy (HC) over all words, and then derives the perplexity from that aggregate value. This gives a corpus-level measure of perplexity.\n",
    "  \n",
    "  - **`compute_perplexity_bigram_avg`:**  \n",
    "    Here, the function calculates the cross-entropy and corresponding perplexity for each individual sentence. After computing each sentence’s perplexity, it averages these perplexities over all sentences. This approach treats each sentence independently before averaging the results.\n",
    "\n",
    "- **Implications of the Aggregation Method:**  \n",
    "  Because perplexity is derived via an exponentiation of cross-entropy (i.e., \\( \\text{Perplexity} = 2^{HC} \\)), which is a non-linear transformation, averaging sentence-level perplexities does not yield the same result as computing perplexity over the entire corpus. Differences in sentence lengths and variance in probability distributions across sentences can lead to these discrepancies.\n",
    "\n",
    "In summary, the first function provides an overall measure for the entire dataset, and the second function gives an average of per-sentence perplexities, which may differ due to the non-linear nature of the transformation from cross-entropy to perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The same logic for the other N-gram prob models_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram probability model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the trigram probabilities\n",
    "def compute_trigram_probabilities(w1, w2, w3, ngram_counts, alpha):\n",
    "    \"\"\"\n",
    "    Computes smoothed trigram probability:\n",
    "      P(w3|w1, w2) = (C(w1, w2, w3) + alpha) / (C(w1, w2) + alpha * |V|)\n",
    "    \"\"\"\n",
    "    trigram_count = ngram_counts[3][(w1, w2, w3)]\n",
    "    bigram_count = ngram_counts[2][(w1, w2)]\n",
    "    \n",
    "    V = len(ngram_counts[1])\n",
    "    \n",
    "    trigram_probability = (trigram_count + alpha) / (bigram_count + alpha * V)\n",
    "    \n",
    "    return trigram_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_trigram(sentences, ngram_counts, alpha):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of the validation set using the trigram model.\n",
    "    \"\"\"\n",
    "    total_log_prob = 0\n",
    "    total_words = 0\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Computing Perplexity\"):\n",
    "        for i in range(2, len(sentence)):\n",
    "            w1 = sentence[i - 2]\n",
    "            w2 = sentence[i - 1]\n",
    "            w3 = sentence[i]\n",
    "            trigram_prob = compute_trigram_probabilities(w1, w2, w3, ngram_counts, alpha)\n",
    "            log_prob = np.log2(trigram_prob)\n",
    "            total_log_prob += log_prob\n",
    "            total_words += 1\n",
    "\n",
    "    HC = -total_log_prob / total_words\n",
    "    perplexity = math.pow(2, HC)\n",
    "\n",
    "    return HC, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolated bi-gram and tri-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_interpolated_prob(w1, w2, w3, ngram_counts, alpha, lamda):\n",
    "    \"\"\"\n",
    "    Computes the interpolated probability:\n",
    "      P(w3|w1,w2) = lam * P_trigram(w3|w1,w2) + (1 - lam) * P_bigram(w3|w2)\n",
    "    where the bigram probability is computed as:\n",
    "      P(w3|w2) = (C(w2, w3) + alpha) / (C(w2) + alpha * |V|)\n",
    "    \"\"\"\n",
    "    # Trigram probability\n",
    "    p_trigram = compute_trigram_probabilities(w1, w2, w3, ngram_counts, alpha)\n",
    "    \n",
    "    # Bigram probability\n",
    "    p_bigram = compute_bigram_probabilities(w2, w3, ngram_counts, alpha)\n",
    "    \n",
    "    # Interpolated probability\n",
    "    interpolated_prob = lamda * p_trigram + (1 - lamda) * p_bigram\n",
    "    \n",
    "    return interpolated_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_interpolated(sentences, ngram_counts, alpha, lamda):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of the validation set using the interpolated model.\n",
    "    \"\"\"\n",
    "    total_log_prob = 0\n",
    "    total_words = 0\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Computing Perplexity\"):\n",
    "        for i in range(2, len(sentence)):\n",
    "            w1 = sentence[i - 2]\n",
    "            w2 = sentence[i - 1]\n",
    "            w3 = sentence[i]\n",
    "            interpolated_prob = compute_interpolated_prob(w1, w2, w3, ngram_counts, alpha, lamda)\n",
    "            log_prob = np.log2(interpolated_prob)\n",
    "            total_log_prob += log_prob\n",
    "            total_words += 1\n",
    "\n",
    "    HC = -total_log_prob / total_words\n",
    "    perplexity = math.pow(2, HC)\n",
    "\n",
    "    return HC, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_interpolated_avg(sentences, ngram_counts, alpha, lamda):\n",
    "    \"\"\"\n",
    "    Compute the average perplexity of the validation set using the interpolated model.\n",
    "    \"\"\"\n",
    "    total_perplexity = 0\n",
    "    num_sentences = len(sentences)\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Computing Perplexity\"):\n",
    "        total_log_prob = 0\n",
    "        total_words = 0\n",
    "        for i in range(2, len(sentence)):\n",
    "            w1 = sentence[i - 2]\n",
    "            w2 = sentence[i - 1]\n",
    "            w3 = sentence[i]\n",
    "            interpolated_prob = compute_interpolated_prob(w1, w2, w3, ngram_counts, alpha, lamda)\n",
    "            log_prob = np.log2(interpolated_prob)\n",
    "            total_log_prob += log_prob\n",
    "            total_words += 1\n",
    "\n",
    "        HC = -total_log_prob / total_words\n",
    "        sentence_perplexity = math.pow(2, HC)\n",
    "        total_perplexity += sentence_perplexity\n",
    "\n",
    "    average_perplexity = total_perplexity / num_sentences\n",
    "\n",
    "    return HC, average_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General N-gram probability model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngram_probability(context, candidate, ngram_counts, alpha, V):\n",
    "    \"\"\"\n",
    "    Computes smoothed n-gram probability:\n",
    "      P(candidate|context) = (C(context, candidate) + alpha) / (C(context) + alpha * |V|)\n",
    "    \"\"\"\n",
    "    context_length = len(context)\n",
    "    ngram_count = ngram_counts.get(context_length + 1, {}).get(context + (candidate,), 0)\n",
    "    context_count = ngram_counts.get(context_length, {}).get(context, 0)\n",
    "    V_size = len(V)\n",
    "    \n",
    "    ngram_probability = (ngram_count + alpha) / (context_count + alpha * V_size)\n",
    "    \n",
    "    return ngram_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After introducing the bigram, trigram, and interpolated models, I proceeded to tune the hyperparameters—specifically the alpha values (smoothing parameters) and lambda values (for interpolation). By iterating over a range of alpha values for the bigram and trigram models and both alpha and lambda values for the interpolated model on a validation set of 10,000 sentences, I aimed to identify the parameter combinations that minimized cross-entropy and perplexity. This systematic tuning process helps ensure that the language models produce more accurate probability estimates and better capture the nuances of the training data, ultimately leading to improved performance on unseen text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyperparameters for the bigram LM:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:01<00:00, 5044.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001, Cross-Entropy: 15.571766524590013, Perplexity: 48704.46861441161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:01<00:00, 5151.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Cross-Entropy: 14.330228241264058, Perplexity: 20598.16559884154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:01<00:00, 5164.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Cross-Entropy: 13.369777434082627, Perplexity: 10585.321255172768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:01<00:00, 5141.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, Cross-Entropy: 13.225124444839032, Perplexity: 9575.449149327107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4936.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.2, Cross-Entropy: 13.332784876705192, Perplexity: 10317.350239208281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:01<00:00, 5026.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.3, Cross-Entropy: 13.413407114210472, Perplexity: 10910.32996801804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:01<00:00, 5213.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5, Cross-Entropy: 13.525765811997951, Perplexity: 11794.002723707026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:01<00:00, 5114.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0, Cross-Entropy: 13.684862012267859, Perplexity: 13169.03500911066\n",
      "Tuning hyperparameters for the trigram LM:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4065.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001, Cross-Entropy: 14.620505569248602, Perplexity: 25188.98849920805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4396.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Cross-Entropy: 14.206140808074235, Perplexity: 18900.55277083627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4244.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Cross-Entropy: 14.159908223480418, Perplexity: 18304.46800107179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4483.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, Cross-Entropy: 14.215964158776348, Perplexity: 19029.686298875287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4257.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.2, Cross-Entropy: 14.236179841975247, Perplexity: 19298.215691654135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4436.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.3, Cross-Entropy: 14.247777104562108, Perplexity: 19453.971710877144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4391.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5, Cross-Entropy: 14.261719897904568, Perplexity: 19642.89427053955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:02<00:00, 4448.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0, Cross-Entropy: 14.278660100343172, Perplexity: 19874.90164292532\n",
      "Tuning hyperparameters for the interpolated LM:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3003.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001, Lambda: 0.1, Cross-Entropy: 13.74841975635381, Perplexity: 13762.16434002628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3085.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001, Lambda: 0.2, Cross-Entropy: 13.421845623129277, Perplexity: 10974.332889969628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3076.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001, Lambda: 0.3, Cross-Entropy: 13.251782886276027, Perplexity: 9754.031298146649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3071.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001, Lambda: 0.5, Cross-Entropy: 13.104681697956275, Perplexity: 8808.506410006923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3106.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001, Lambda: 0.7, Cross-Entropy: 13.126402758739374, Perplexity: 8942.129716459292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3051.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001, Lambda: 0.9, Cross-Entropy: 13.420482816570608, Perplexity: 10963.971149767824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2952.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Lambda: 0.1, Cross-Entropy: 13.388771662904029, Perplexity: 10725.606896728907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3092.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Lambda: 0.2, Cross-Entropy: 13.133015829305382, Perplexity: 8983.213017329448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3108.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Lambda: 0.3, Cross-Entropy: 12.990919827534746, Perplexity: 8140.602319926089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3092.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Lambda: 0.5, Cross-Entropy: 12.865048054124225, Perplexity: 7460.455769840212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3108.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Lambda: 0.7, Cross-Entropy: 12.889988452461294, Perplexity: 7590.5486469191455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2993.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Lambda: 0.9, Cross-Entropy: 13.170092411617906, Perplexity: 9217.069485687693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3098.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Lambda: 0.1, Cross-Entropy: 13.054219315058122, Perplexity: 8505.72982676796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2903.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Lambda: 0.2, Cross-Entropy: 12.95266768545199, Perplexity: 7927.595768650709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2895.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Lambda: 0.3, Cross-Entropy: 12.898792610216251, Perplexity: 7637.012184923165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3069.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Lambda: 0.5, Cross-Entropy: 12.874492605207696, Perplexity: 7509.455587400954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2707.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Lambda: 0.7, Cross-Entropy: 12.956232928853806, Perplexity: 7947.210974771716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3135.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Lambda: 0.9, Cross-Entropy: 13.264261707383362, Perplexity: 9838.766285362026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3074.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, Lambda: 0.1, Cross-Entropy: 13.171630429860954, Perplexity: 9226.900794201996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3035.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, Lambda: 0.2, Cross-Entropy: 13.165096248539044, Perplexity: 9185.205276259956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3094.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, Lambda: 0.3, Cross-Entropy: 13.176780061465879, Perplexity: 9259.894629423752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3134.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, Lambda: 0.5, Cross-Entropy: 13.243434492534057, Perplexity: 9697.750975674371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2921.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, Lambda: 0.7, Cross-Entropy: 13.381695482825537, Perplexity: 10673.128376878805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2810.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, Lambda: 0.9, Cross-Entropy: 13.688399665414488, Perplexity: 13201.366612424476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3093.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.2, Lambda: 0.1, Cross-Entropy: 13.313993041498236, Perplexity: 10183.832968208546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3032.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.2, Lambda: 0.2, Cross-Entropy: 13.322878081900287, Perplexity: 10246.745064096416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2722.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.2, Lambda: 0.3, Cross-Entropy: 13.344439653685741, Perplexity: 10401.036276166316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3096.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.2, Lambda: 0.5, Cross-Entropy: 13.422026880278523, Perplexity: 10975.711768527679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2755.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.2, Lambda: 0.7, Cross-Entropy: 13.560104693680929, Perplexity: 12078.089880640742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3124.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.2, Lambda: 0.9, Cross-Entropy: 13.836544304785765, Perplexity: 14629.007820328075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3059.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.3, Lambda: 0.1, Cross-Entropy: 13.408380719477208, Perplexity: 10872.38417705044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3082.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.3, Lambda: 0.2, Cross-Entropy: 13.423816716442913, Perplexity: 10989.336904925754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3075.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.3, Lambda: 0.3, Cross-Entropy: 13.449282648171627, Perplexity: 11185.03885910331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2978.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.3, Lambda: 0.5, Cross-Entropy: 13.529699897031351, Perplexity: 11826.207679021836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3116.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.3, Lambda: 0.7, Cross-Entropy: 13.66336565997831, Perplexity: 12974.269218489815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3077.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.3, Lambda: 0.9, Cross-Entropy: 13.915883410354882, Perplexity: 15456.04319086476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3131.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5, Lambda: 0.1, Cross-Entropy: 13.532514455118235, Perplexity: 11849.301983139223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3095.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5, Lambda: 0.2, Cross-Entropy: 13.553571800189744, Perplexity: 12023.520833735003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3046.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5, Lambda: 0.3, Cross-Entropy: 13.581963683365942, Perplexity: 12262.48442318979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3097.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5, Lambda: 0.5, Cross-Entropy: 13.662285258617828, Perplexity: 12964.55672183208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3107.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5, Lambda: 0.7, Cross-Entropy: 13.786314718599488, Perplexity: 14128.44157281574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3093.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5, Lambda: 0.9, Cross-Entropy: 14.004445186616572, Perplexity: 16434.559717363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:04<00:00, 2428.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0, Lambda: 0.1, Cross-Entropy: 13.699871746504796, Perplexity: 13306.760257644217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2692.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0, Lambda: 0.2, Cross-Entropy: 13.724104298991048, Perplexity: 13532.15792823409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 3016.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0, Lambda: 0.3, Cross-Entropy: 13.753030375680103, Perplexity: 13806.216338878241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2884.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0, Lambda: 0.5, Cross-Entropy: 13.82730146463481, Perplexity: 14535.584492750848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2976.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0, Lambda: 0.7, Cross-Entropy: 13.932713511762609, Perplexity: 15637.40513465185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Perplexity: 100%|██████████| 10000/10000 [00:03<00:00, 2822.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0, Lambda: 0.9, Cross-Entropy: 14.101701637133313, Perplexity: 17580.66031082511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tune the hyperparameters for the bigram LM\n",
    "validation_set = corpus_sentences[:10000]\n",
    "alpha_values = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.5, 1.0]\n",
    "lambda_values = [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "best_bigram_params=None\n",
    "best_bigram_ce = float('inf')\n",
    "print(\"Tuning hyperparameters for the bigram LM:\")\n",
    "for alpha in (alpha_values):\n",
    "    HC, perpl = compute_perplexity_bigram(validation_set, ngram_counts, alpha)\n",
    "    print(f\"Alpha: {alpha}, Cross-Entropy: {HC}, Perplexity: {perpl}\")\n",
    "    if HC < best_bigram_ce:\n",
    "        best_bigram_ce = HC\n",
    "        best_bigram_params = (alpha, HC, perpl)\n",
    "        \n",
    "# Tune the hyperparameters for the trigram LM\n",
    "best_trigram_params=None\n",
    "best_trigram_ce = float('inf')\n",
    "print(\"Tuning hyperparameters for the trigram LM:\")\n",
    "for alpha in (alpha_values):\n",
    "    HC, perpl = compute_perplexity_trigram(validation_set, ngram_counts, alpha)\n",
    "    print(f\"Alpha: {alpha}, Cross-Entropy: {HC}, Perplexity: {perpl}\")\n",
    "    if HC < best_trigram_ce:\n",
    "        best_trigram_ce = HC\n",
    "        best_trigram_params = (alpha, HC, perpl)\n",
    "        \n",
    "# Tune the hyperparameters for the interpolated LM\n",
    "best_interpolated_params=None\n",
    "best_interpolated_ce = float('inf')\n",
    "print(\"Tuning hyperparameters for the interpolated LM:\")\n",
    "for alpha in (alpha_values):\n",
    "    for lamda in lambda_values:\n",
    "        HC, perpl = compute_perplexity_interpolated(validation_set, ngram_counts, alpha, lamda)\n",
    "        print(f\"Alpha: {alpha}, Lambda: {lamda}, Cross-Entropy: {HC}, Perplexity: {perpl}\")\n",
    "        if HC < best_interpolated_ce:\n",
    "            best_interpolated_ce = HC\n",
    "            best_interpolated_params = (alpha, lamda, HC, perpl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for the bigram LM:\n",
      "Alpha: 0.1, Cross-Entropy: 13.225124444839032, Perplexity: 9575.449149327107\n",
      "Best hyperparameters for the trigram LM:\n",
      "Alpha: 0.01, Cross-Entropy: 14.159908223480418, Perplexity: 18304.46800107179\n",
      "Best hyperparameters for the interpolated LM:\n",
      "Alpha: 0.001, Lambda: 0.5, Cross-Entropy: 12.865048054124225, Perplexity: 7460.455769840212\n"
     ]
    }
   ],
   "source": [
    "# Print best paraneters\n",
    "print(\"Best hyperparameters for the bigram LM:\")\n",
    "print(f\"Alpha: {best_bigram_params[0]}, Cross-Entropy: {best_bigram_params[1]}, Perplexity: {best_bigram_params[2]}\")\n",
    "print(\"Best hyperparameters for the trigram LM:\")\n",
    "print(f\"Alpha: {best_trigram_params[0]}, Cross-Entropy: {best_trigram_params[1]}, Perplexity: {best_trigram_params[2]}\")\n",
    "print(\"Best hyperparameters for the interpolated LM:\")\n",
    "print(f\"Alpha: {best_interpolated_params[0]}, Lambda: {best_interpolated_params[1]}, Cross-Entropy: {best_interpolated_params[2]}, Perplexity: {best_interpolated_params[3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best hyperparameters\n",
    "best_hyperparameters = {\n",
    "    \"bigram\": best_bigram_params,\n",
    "    \"trigram\": best_trigram_params,\n",
    "    \"interpolated\": best_interpolated_params\n",
    "}\n",
    "\n",
    "with open('data/train/best_hyperparameters.json', 'w') as f:\n",
    "    json.dump(best_hyperparameters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context-Sensitive Correction based on N-gram model and beam seacrh approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I collect everything into a single system for Context-Sensitive Correction with beam seacrh approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = None\n",
    "with open('data/train/best_hyperparameters.json', 'r') as f:\n",
    "    best_hyperparameters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best hyperparameters for the models\n",
    "best_bigram_alpha = best_hyperparameters['bigram'][0]\n",
    "best_trigram_alpha = best_hyperparameters['trigram'][0]\n",
    "best_interpolated_alpha = best_hyperparameters['interpolated'][0]\n",
    "best_interpolated_lambda = best_hyperparameters['interpolated'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_candidates(word, V):\n",
    "    def known(words, V):\n",
    "        return set(w for w in words if w in V)\n",
    "    \n",
    "    def edits1(word):\n",
    "        \"All edits that are one edit away from `word`.\"\n",
    "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "    \n",
    "    def edits2(word):\n",
    "        \"All edits that are two edits away from `word`.\"\n",
    "        return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "    \n",
    "    def edits3(word):\n",
    "        \"All edits that are three edits away from `word`.\"\n",
    "        return (e3 for e2 in edits1(word) for e3 in edits1(e2))\n",
    "    \n",
    "    return known([word], V) or known(edits1(word), V) or known(edits2(word), V) or known(edits3(word), V) or {word}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updated code leverages the `@lru_cache` decorator to cache results from the `get_candidates` function, significantly speeding up repeated lookups for the same word and vocabulary. Within this function, several helper functions are defined: \n",
    "\n",
    "- **`known(words, V)`:** Filters a list of words to include only those present in the vocabulary.  \n",
    "- **`edits1(word)`:** Generates all possible words that are one edit away from the input word by performing deletions, transpositions, replacements, and insertions.  \n",
    "- **`edits2(word)` and `edits3(word)`:** Extend this idea by generating candidates that are two or three edits away, respectively.\n",
    "\n",
    "This hierarchical candidate generation process ensures that if no direct or one-edit correction is found, the function can still consider more distant alternatives, thereby increasing the robustness of the correction mechanism.\n",
    "\n",
    "In comparison to the original implementation—which encapsulated methods like probability computation (`P`), candidate generation, and correction within a class—the improved version modularizes these functionalities and optimizes performance with caching. The addition of the `edits3` function further extends the search space for potential corrections. These improvements not only enhance clarity and maintainability but also lead to more efficient computation, especially when handling repeated queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq\n",
    "\n",
    "def corrector_with_beam_search(sentence, V, ngram_counts, beam_width=2, n=3, alpha=0.01, lamda=0.5):\n",
    "    # Preprocess the input sentence and tokenize it\n",
    "    sentence_tokens = preprocess_sentences([sentence])[0]\n",
    "    \n",
    "    # Initialize the list of candidate sequences with an initial score of 0.0 and an empty sequence\n",
    "    candidates_sequences = [(0.0, [])]\n",
    "    \n",
    "    # Total number of words in the vocabulary\n",
    "    total_words = len(V)\n",
    "    \n",
    "    # Iterate over each word in the tokenized sentence\n",
    "    for word in sentence_tokens:\n",
    "        new_candidates_sequences = []\n",
    "        \n",
    "        # Get the list of candidate words for the current word\n",
    "        candidate_list = get_candidates(word, V)\n",
    "        \n",
    "        # Iterate over each candidate word\n",
    "        for candidate in candidate_list:\n",
    "            # Iterate over each candidate sequence\n",
    "            for score, sequence in candidates_sequences:\n",
    "                # Determine the context length for n-gram probability calculation\n",
    "                context_length = min(n - 1, len(sequence))\n",
    "                \n",
    "                # Compute the probability based on the context length\n",
    "                if context_length == 0:\n",
    "                    # Unigram probability\n",
    "                    prob = compute_unigram_probability(candidate, ngram_counts, alpha, total_words, V)\n",
    "                elif context_length == 1:\n",
    "                    # Bigram probability\n",
    "                    context = tuple(sequence[-1:])\n",
    "                    prob = compute_bigram_probabilities(context[0], candidate, ngram_counts, best_bigram_alpha)\n",
    "                elif context_length == 2:\n",
    "                    # Trigram probability with interpolation\n",
    "                    w1, w2 = sequence[-2], sequence[-1]\n",
    "                    prob = compute_interpolated_prob(w1, w2, candidate, ngram_counts, best_interpolated_alpha, best_interpolated_lambda)\n",
    "                else:\n",
    "                    # General n-gram probability\n",
    "                    context = tuple(sequence[-context_length:])\n",
    "                    prob = compute_ngram_probability(context, candidate, ngram_counts, alpha, V)\n",
    "                \n",
    "                # Update the score with the log probability\n",
    "                new_score = score + math.log(prob)\n",
    "                \n",
    "                # Create a new sequence by appending the candidate word\n",
    "                new_seq = sequence + [candidate]\n",
    "                \n",
    "                # Add the new candidate sequence to the heap\n",
    "                heapq.heappush(new_candidates_sequences, (new_score, new_seq))\n",
    "                \n",
    "                # Ensure the heap does not exceed the beam width\n",
    "                if len(new_candidates_sequences) > beam_width:\n",
    "                    heapq.heappop(new_candidates_sequences)\n",
    "        \n",
    "        # Update the list of candidate sequences for the next iteration\n",
    "        candidates_sequences = new_candidates_sequences\n",
    "    \n",
    "    # Select the best sequence with the highest score\n",
    "    best_score, best_seq = max(candidates_sequences, key=lambda x: x[0])\n",
    "    \n",
    "    # Return the best sequence as a single string\n",
    "    return ' '.join(best_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct vocab into a frozenset for the lrucache decorator\n",
    "vocabulary = frozenset(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'she is king to the per'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test example for unigram\n",
    "text = \"Seh is ging t te perk\"\n",
    "corrector_with_beam_search(text, vocabulary, ngram_counts, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6990.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'she is king sport'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test example for unigram\n",
    "text = \"Seh is dking sport\"\n",
    "corrector_with_beam_search(text, vocabulary, ngram_counts, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3483.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'she is going to the park'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test example for bigram\n",
    "text = \"Seh is ging t te perk\"\n",
    "corrector_with_beam_search(text, vocabulary, ngram_counts, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5915.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'she is doing sport'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test example for bigram\n",
    "text = \"Seh is dking sport\"\n",
    "corrector_with_beam_search(text, vocabulary, ngram_counts, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7049.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'she is going to the park'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test example for trigram\n",
    "text = \"Seh is ging t te perk\"\n",
    "corrector_with_beam_search(text, vocabulary, ngram_counts, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3609.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'she is going tx ye peru'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test example for N-gram model\n",
    "text = \"Seh is ging t te perk\"\n",
    "corrector_with_beam_search(text, vocabulary, ngram_counts, n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our tests, we applied beam search with different n-gram models to correct the sentence \"Seh is ging t te perk\". \n",
    "\n",
    "With the unigram model (n=1), the output was \"she is king to the per\". Here, the model relies solely on individual word frequencies, which often leads to inappropriate substitutions without context. Switching to the bigram model (n=2) produced \"she is going to the park\", demonstrating a significant improvement by taking into account the immediate context of adjacent words. \n",
    "\n",
    "However, when we moved to the trigram model (n=3), the output degraded to \"she is going tx ye peru\". This suggests that while adding context generally improves performance, using higher-order n-grams can suffer from data sparsity or overfitting issues if there isn’t enough reliable contextual data. In summary, the bigram and interpol model strikes a good balance, effectively using contextual information to enhance accuracy without the pitfalls encountered at higher n-gram orders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computes metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions to calculate WER and CER\n",
    "def calculate_wer(reference, corrected):\n",
    "    # Calculate Word Error Rate (WER)\n",
    "    reference_words = reference.split()\n",
    "    corrected_words = corrected.split()\n",
    "\n",
    "    S = Levenshtein(reference_words, corrected_words)\n",
    "    I = max(0, len(corrected_words) - len(reference_words))\n",
    "    D = max(0, len(reference_words) - len(corrected_words))\n",
    "\n",
    "    N = max(len(reference_words), len(corrected_words))\n",
    "\n",
    "    wer = (S + I + D) / N\n",
    "\n",
    "    return wer\n",
    "\n",
    "def calculate_cer(reference, corrected):\n",
    "    # Calculate Character Error Rate (CER)\n",
    "    S = Levenshtein(reference, corrected)\n",
    "    I = max(0, len(corrected) - len(reference))\n",
    "    D = max(0, len(reference) - len(corrected))\n",
    "\n",
    "    N = max(len(reference), len(corrected))\n",
    "\n",
    "    cer = (S + I + D) / N\n",
    "\n",
    "    return cer\n",
    "\n",
    "def compute_metrics_for_ngram_models(df, vocabulary, ngram_counts, n = 3):\n",
    "    WER = []\n",
    "    CER = []\n",
    "    \n",
    "    corrected_sentences = []\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    # Correct the sentences using the beam search corrector\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        misspelled_sentence = row['Misspelled Sentence']\n",
    "        reference = row['Original Sentence']\n",
    "        corrected_sentence = corrector_with_beam_search(misspelled_sentence, vocabulary, ngram_counts, n=n)\n",
    "        corrected_sentences.append(corrected_sentence)\n",
    "                \n",
    "        # Calculate WER and CER\n",
    "        wer = calculate_wer(reference, corrected_sentence)\n",
    "        cer = calculate_cer(reference, corrected_sentence)\n",
    "        WER.append(wer)\n",
    "        CER.append(cer)\n",
    "        \n",
    "        # Compute the accuracy\n",
    "        list_of_corrected_words = ast.literal_eval(row['Correct Words'])\n",
    "        for word in list_of_corrected_words:\n",
    "            if word in corrected_sentence:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "                    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    if n == 2:\n",
    "        sentences = corrected_sentences\n",
    "        HC, perplexity = compute_perplexity_bigram_avg(sentences, ngram_counts, best_bigram_alpha)\n",
    "        print(f\"Perplexity: {perplexity}\")\n",
    "        print(f\"Cross-Entropy: {HC}\")\n",
    "    if n == 3:\n",
    "        sentences = corrected_sentences\n",
    "        HC, perplexity = compute_perplexity_interpolated_avg(sentences, ngram_counts, best_interpolated_alpha, best_interpolated_lambda)\n",
    "        print(f\"Perplexity: {perplexity}\")\n",
    "        print(f\"Cross-Entropy: {HC}\")\n",
    "        \n",
    "        \n",
    "    print('Average Word Error Rate (WER):', np.mean(WER))\n",
    "    print('Average Character Error Rate (CER):', np.mean(CER))\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    df[\"Corrected Sentence\"] = corrected_sentences\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolated model perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10727.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8701.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5949.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5178.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3905.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5706.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5210.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5440.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5504.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12787.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12157.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5991.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13025.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4975.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13842.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4877.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6533.18it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10512.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4293.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4629.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3463.50it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4854.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3054.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3057.07it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5817.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8774.69it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2543.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6754.11it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14926.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5433.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10106.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11008.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4744.69it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6574.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9118.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9000.65it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12336.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4148.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3390.71it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14665.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9000.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9177.91it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2251.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15363.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5289.16it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12018.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11214.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2761.23it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5203.85it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5096.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4739.33it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5322.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6754.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8701.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6132.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9868.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14873.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6413.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14513.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15592.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10894.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3701.95it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12671.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15363.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8405.42it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5907.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13934.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11881.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8830.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9822.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10106.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8338.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7810.62it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2918.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3968.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4559.03it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8128.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9425.40it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4899.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12671.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9709.04it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5295.84it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3486.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11244.78it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3415.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5203.85it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5165.40it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12336.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8050.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3609.56it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5152.71it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9258.95it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4315.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15363.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9915.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8559.80it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11366.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6177.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14266.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5236.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8272.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11244.78it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14513.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5714.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8594.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8774.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7410.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12157.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2900.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6213.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5497.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3039.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8473.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6061.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8473.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4568.96it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6017.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10180.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9597.95it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3266.59it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4405.78it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13842.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5356.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 16384.00it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3216.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3134.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4832.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7810.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14926.35it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12157.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10727.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13662.23it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13934.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4826.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4650.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5023.12it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15827.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4350.94it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15592.21it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4696.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13357.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8793.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7084.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6543.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11008.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4650.00it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7256.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13934.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12336.19it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6853.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3659.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5849.80it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3819.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4718.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13662.23it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14266.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1661.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12300.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10894.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14563.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3095.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4048.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12787.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9098.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13842.59it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14463.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6213.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4382.76it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7695.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5377.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14217.98it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15650.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 654.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5377.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12671.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5047.30it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3609.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6533.18it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5377.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4036.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5555.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12483.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10837.99it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5849.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12336.19it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9020.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5737.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12865.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5652.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10485.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9157.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9425.40it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9258.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1938.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1647.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2974.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3289.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14027.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14463.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6374.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6335.81it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6017.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3890.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6026.30it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7332.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9177.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8612.53it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9915.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3279.36it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7710.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2118.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7358.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4032.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5614.86it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14463.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13357.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4236.67it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13486.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6061.13it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4854.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3236.35it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14027.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15363.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7244.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3775.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4519.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9597.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15887.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7752.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9020.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5753.50it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2139.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5497.12it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14122.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3609.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3426.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3862.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4021.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10894.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5178.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5622.39it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14463.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3983.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7096.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12228.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14979.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7358.43it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10106.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14266.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4999.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3744.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8144.28it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11008.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4328.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5983.32it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12336.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11125.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11244.78it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11650.84it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6096.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11881.88it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7358.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1730.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13842.59it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10837.99it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11366.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10837.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9177.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4144.57it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4718.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2803.68it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13486.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2966.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5203.85it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13486.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14979.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15650.39it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6096.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4364.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5714.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4568.96it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14513.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3452.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14122.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5991.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9238.56it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4782.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13486.51it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4675.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11008.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3744.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7584.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3344.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7695.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3890.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10866.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8774.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7516.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6842.26it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7206.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4293.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9619.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7463.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4650.00it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9776.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3472.11it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5957.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5683.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11781.75it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14513.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12300.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10131.17it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3983.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6442.86it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14926.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 16131.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4777.11it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2555.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7516.67it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7869.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13486.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4198.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8388.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4471.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4739.33it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4519.72it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6754.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5907.47it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12671.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11366.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10727.12it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6533.18it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5841.65it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6932.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3196.88it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13357.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10837.99it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14926.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4634.59it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4583.94it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9619.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13486.51it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4202.71it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11244.78it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4148.67it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7410.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5497.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6326.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5210.32it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13357.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6168.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12787.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6442.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9892.23it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11008.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4505.16it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5461.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 753.56it/s]/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11366.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4951.95it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12671.61it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13443.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 597.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10512.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3701.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10512.04it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3637.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9000.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3609.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6087.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8542.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6853.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6721.64it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6241.52it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8272.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5949.37it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2716.52it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1900.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2807.43it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5915.80it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11335.96it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7626.01it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9731.56it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6326.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7397.36it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10010.27it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10512.04it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9000.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12300.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9078.58it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2652.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6978.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6250.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10591.68it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12087.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8774.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6574.14it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7530.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12945.38it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9362.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13934.56it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12300.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6808.94it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12018.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11522.81it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5785.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5047.30it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4236.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8256.50it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13842.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12671.61it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8473.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9709.04it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6087.52it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3495.25it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5882.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10618.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11214.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11781.75it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6533.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14266.34it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12018.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11715.93it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5817.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5622.39it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5957.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8793.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9177.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6898.53it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2646.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4718.00it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8256.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5548.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10512.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9118.05it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4462.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6374.32it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9425.40it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11881.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13486.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7463.17it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10618.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13189.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15887.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14266.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5433.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10618.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11008.67it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4236.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14266.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2058.05it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2511.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6584.46it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4922.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13934.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3287.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3536.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4549.14it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4951.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5405.03it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10082.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8208.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11214.72it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5584.96it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4691.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9822.73it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4675.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9619.96it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6853.44it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8542.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10082.46it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11366.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5210.32it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6326.25it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5817.34it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4848.91it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9822.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11397.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9799.78it/s]\n",
      "100%|██████████| 666/666 [01:28<00:00,  7.51it/s]\n",
      "Computing Perplexity: 100%|██████████| 666/666 [00:00<00:00, 2323.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 98372.6070522449\n",
      "Cross-Entropy: 16.585974779349506\n",
      "Average Word Error Rate (WER): 0.1859610635597018\n",
      "Average Character Error Rate (CER): 0.08163519000092576\n",
      "Accuracy: 0.3605232781839169\n"
     ]
    }
   ],
   "source": [
    "updated = compute_metrics_for_ngram_models(test_df.copy(), vocabulary, ngram_counts, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misspelled Sentence: 1 nigel thrush page 48 i have four in my family dad mum and siter\n",
      "Misspleed Words: ['siter']\n",
      "Corrected: m1 nigel thrust page 48 in have four in my family dad sum and sister\n",
      "Target: 1 nigel thrush page 48 i have four in my family dad mum and sister\n",
      "-----------------------------\n",
      "Misspelled Sentence: my siter go to tonbury\n",
      "Misspleed Words: ['siter', 'go']\n",
      "Corrected: my sister go to tonbury\n",
      "Target: my sister goes to tonbury\n",
      "-----------------------------\n",
      "Misspelled Sentence: my mum goes out sometimes\n",
      "Misspleed Words: ['sometimes']\n",
      "Corrected: my um goes out sometimes\n",
      "Target: my mum goes out sometimes\n",
      "-----------------------------\n",
      "Misspelled Sentence: i go to bridgebrook i go out sometimes on tuesday night i go to youth clob\n",
      "Misspleed Words: ['sometimes', 'clob']\n",
      "Corrected: it go to bridgebrook xi go out sometimes on tuesday night in go to youth club\n",
      "Target: i go to bridgebrook i go out sometimes on tuesday night i go to youth club\n",
      "-----------------------------\n",
      "Misspelled Sentence: on thursday nights i go bellringing on saturdays i go down to the farm\n",
      "Misspleed Words: ['bellringing']\n",
      "Corrected: on thursday nights it go bellringing on saturday it go down to the farm\n",
      "Target: on thursday nights i go bellringing on saturdays i go down to the farm\n",
      "-----------------------------\n",
      "Misspelled Sentence: i go to bed at 10 o clock i wakh tv at 5 o clock i live in a house\n",
      "Misspleed Words: ['wakh']\n",
      "Corrected: it go to bed at 10 to clock is wake tv at 5p to clock in live in an house\n",
      "Target: i go to bed at 10 o clock i watch tv at 5 o clock i live in a house\n",
      "-----------------------------\n",
      "Misspelled Sentence: the house is white it has stone up the frount it is the first from bridgebrook and the sexeon from smallerden\n",
      "Misspleed Words: ['frount', 'sexeon']\n",
      "Corrected: the house is white it has stone up the front it is the first from bridgebrook and the season from smallerden\n",
      "Target: the house is white it has stone up the front it is the first from bridgebrook and the second from smallerden\n",
      "-----------------------------\n",
      "Misspelled Sentence: on monday i sometimes go down the farm in the night i wach tv there is bbc and itv\n",
      "Misspleed Words: ['wach']\n",
      "Corrected: on monday it sometimes go down the farm in the night in each tv there is bbc and it\n",
      "Target: on monday i sometimes go down the farm in the night i watch tv there is bbc and itv\n",
      "-----------------------------\n",
      "Misspelled Sentence: we have got anglia like to wach cowboys\n",
      "Misspleed Words: ['wach', 'cowboys']\n",
      "Corrected: we have got anglican like to each cowboy\n",
      "Target: we have got anglia like to watch cowboys\n",
      "-----------------------------\n",
      "Misspelled Sentence: on tuesday i get off the bus and sometimes in the night i go to the youth colbe\n",
      "Misspleed Words: ['sometimes', 'colbe']\n",
      "Corrected: on tuesday it get off the bus and sometimes in the night it go to the youth cole\n",
      "Target: on tuesday i get off the bus and sometimes in the night i go to the youth club\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# print examples of fixes\n",
    "for i in range(10):\n",
    "    print(f\"Misspelled Sentence: {test_df['Misspelled Sentence'][i]}\")\n",
    "    print(f\"Misspleed Words: {test_df['Misspelled Words'][i]}\")\n",
    "    print(f\"Corrected: {updated['Corrected Sentence'][i]}\")\n",
    "    print(f\"Target: {test_df['Original Sentence'][i]}\")\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the interpolated model, we observed a notable increase in accuracy—from **0.21 to 0.36**—indicating that the model now corrects a higher proportion of errors correctly. However, this improvement in accuracy comes with trade-offs in other metrics. Specifically, the interpolated model yielded:\n",
    "\n",
    "- **Perplexity:** 98372.6070522449  \n",
    "- **Cross-Entropy:** 16.585974779349506  \n",
    "- **Average WER:** 0.1859610635597018  \n",
    "- **Average CER:** 0.08163519000092576  \n",
    "- **Accuracy:** 0.3605232781839169  \n",
    "\n",
    "In contrast, our previous unigram model with preprocessing achieved a lower perplexity (4751.99), cross-entropy (10.07), and error rates, but its accuracy was lower. The high perplexity in the interpolated model is largely due to the relatively small corpus, which results in very low probability estimates when incorporating bigram and trigram counts. Additionally, the increased WER, CER, and cross-entropy indicate that the model may be over-correcting—modifying words that are already correct. \n",
    "\n",
    "This trade-off underscores the challenges in balancing rich contextual modeling with the preservation of correctly spelled words, emphasizing the need for a larger corpus or further tuning to optimize overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram model perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1824.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4848.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13842.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6288.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5262.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6141.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4462.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3858.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3748.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6990.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7145.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6502.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2900.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4854.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12787.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7943.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13486.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3194.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7096.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4946.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5777.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2192.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1517.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4951.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12018.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2449.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8943.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9709.04it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6250.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4675.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8848.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10082.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5849.80it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7244.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12483.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4975.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4324.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12483.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8355.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6026.30it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12300.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13842.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5706.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13443.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12710.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4614.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5622.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15363.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11881.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5849.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 16384.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7307.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12865.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13662.23it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5178.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8405.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15420.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12671.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5127.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14122.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15420.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8338.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6574.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14266.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4993.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7570.95it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2091.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6017.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14027.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14463.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5210.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13189.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15592.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13189.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5370.43it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2890.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5915.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4505.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11366.68it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8612.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5991.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8924.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2198.27it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4310.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15827.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4096.00it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11244.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5159.05it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7570.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5210.32it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7869.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3545.48it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6710.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5096.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 16644.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2951.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15363.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13486.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6288.31it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2470.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6615.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14873.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4946.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5777.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10866.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12483.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3953.16it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3446.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10010.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4539.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14217.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6765.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13189.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1883.39it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4387.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9597.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8112.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9822.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5071.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14513.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14979.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15363.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12300.01it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8848.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11881.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14217.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6297.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15887.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7281.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5412.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15887.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6502.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14266.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4877.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7194.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13189.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10512.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14266.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2931.03it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3865.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14926.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4369.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12300.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13357.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7108.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6626.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14463.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15363.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11275.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2611.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5343.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14563.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15592.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5614.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6177.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11781.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11244.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2045.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4080.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5289.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6374.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14513.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6574.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3908.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12787.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5691.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5315.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4999.17it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2661.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5882.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14926.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12945.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4583.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6615.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3575.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10082.46it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12671.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12483.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11881.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6204.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11881.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4712.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7281.78it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2519.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5526.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6710.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6105.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5882.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9020.01it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6017.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13486.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3355.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7681.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12671.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14266.34it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6326.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5526.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2959.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6452.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4629.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6765.01it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8630.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8924.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8830.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13357.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5849.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7049.25it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6626.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14513.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5295.84it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3344.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5295.84it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5882.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6204.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3472.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5322.72it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6288.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12787.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2236.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2993.79it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8256.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9868.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10433.59it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6842.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12336.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1092.84it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5577.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7410.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11781.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9619.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5777.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8272.79it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5622.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9986.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1694.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6710.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15363.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1302.17it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12787.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14979.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1293.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11008.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7928.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7681.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3584.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10512.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10538.45it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10727.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1095.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6754.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 987.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8774.69it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 727.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7037.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 417.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7869.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5053.38it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5714.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14122.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11125.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9000.65it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6492.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5203.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5289.16it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13357.66it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7570.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15592.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11781.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13025.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3934.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12945.38it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10866.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2037.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12228.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6944.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5041.23it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6096.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6168.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5343.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2900.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14563.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14768.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4198.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15363.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12483.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6326.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7695.97it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3949.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3086.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4462.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9258.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9000.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4500.33it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2681.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6177.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5675.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7943.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3509.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5029.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5440.08it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4271.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12483.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4691.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11781.75it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4588.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2985.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12787.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3609.56it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4447.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10727.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5714.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3460.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4044.65it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8542.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6523.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2414.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6403.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4848.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4206.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8192.00it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6710.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7145.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2158.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8559.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7825.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6533.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9799.78it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4999.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13842.59it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7681.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8848.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2227.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6132.02it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6502.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14217.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12865.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5526.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5817.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10894.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15363.75it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5957.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9986.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10538.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13357.66it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9892.23it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4341.93it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5983.32it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4766.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6754.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12945.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5322.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14665.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7752.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14665.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5577.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5398.07it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14122.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13662.23it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4132.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8256.50it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6168.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6423.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1703.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4148.67it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5152.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14122.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3775.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1381.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13662.23it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13189.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14266.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13486.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8473.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13842.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9776.93it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11781.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6250.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12710.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11781.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7695.97it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12710.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4718.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5526.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11335.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14513.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5518.82it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2410.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3423.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12671.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5915.80it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14266.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14979.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4848.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8756.38it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2293.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12446.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5053.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12710.01it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7626.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13486.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7516.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10433.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11244.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6288.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8943.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2898.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7570.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7825.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8405.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7358.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1703.62it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5817.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13357.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14266.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12336.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3865.72it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8774.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2273.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12228.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5809.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6543.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5433.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11781.75it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12671.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14926.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12671.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4899.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13357.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12018.06it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10538.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3498.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10538.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4975.45it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14122.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9597.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9098.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12865.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3187.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13842.59it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4447.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3449.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5785.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5777.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13934.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3146.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11781.75it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3956.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4951.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12865.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12336.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8542.37it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15592.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6626.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5322.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10180.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11459.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12087.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6442.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12787.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4691.62it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6250.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15420.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5714.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4860.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12483.05it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14513.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5405.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6168.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11366.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11366.68it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5159.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4148.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4691.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4860.14it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6132.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12483.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7084.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4048.56it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10106.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4718.00it/s]s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6204.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8701.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2746.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2833.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10837.99it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7598.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8612.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10837.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6052.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7345.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9709.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9362.29it/s]\n",
      "100%|██████████| 666/666 [00:36<00:00, 18.30it/s]\n",
      "Computing Perplexity: 100%|██████████| 666/666 [00:00<00:00, 3333.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 98373.00000000065\n",
      "Cross-Entropy: 16.585974779349506\n",
      "Average Word Error Rate (WER): 0.1869153014145821\n",
      "Average Character Error Rate (CER): 0.08206445300109326\n",
      "Accuracy: 0.3528280107733744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "updated = compute_metrics_for_ngram_models(test_df.copy(), vocabulary, ngram_counts, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note:_ The interpolated model performs a little bit better due to its ability to combine both bigram and trigram probabilities. By interpolating these n-gram models, it effectively leverages additional context, which helps mitigate the issues of data sparsity that often affect higher-order n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misspelled Sentence: 1 nigel thrush page 48 i have four in my family dad mum and siter\n",
      "Misspleed Words: ['siter']\n",
      "Corrected: m1 nigel thrust page 48 in have four in my family dad sum and sister\n",
      "Target: 1 nigel thrush page 48 i have four in my family dad mum and sister\n",
      "-----------------------------\n",
      "Misspelled Sentence: my siter go to tonbury\n",
      "Misspleed Words: ['siter', 'go']\n",
      "Corrected: my sister go to tonbury\n",
      "Target: my sister goes to tonbury\n",
      "-----------------------------\n",
      "Misspelled Sentence: my mum goes out sometimes\n",
      "Misspleed Words: ['sometimes']\n",
      "Corrected: my um goes out sometimes\n",
      "Target: my mum goes out sometimes\n",
      "-----------------------------\n",
      "Misspelled Sentence: i go to bridgebrook i go out sometimes on tuesday night i go to youth clob\n",
      "Misspleed Words: ['sometimes', 'clob']\n",
      "Corrected: it go to bridgebrook xi go out sometimes on tuesday night in go to youth club\n",
      "Target: i go to bridgebrook i go out sometimes on tuesday night i go to youth club\n",
      "-----------------------------\n",
      "Misspelled Sentence: on thursday nights i go bellringing on saturdays i go down to the farm\n",
      "Misspleed Words: ['bellringing']\n",
      "Corrected: on thursday nights it go bellringing on saturday it go down to the farm\n",
      "Target: on thursday nights i go bellringing on saturdays i go down to the farm\n",
      "-----------------------------\n",
      "Misspelled Sentence: i go to bed at 10 o clock i wakh tv at 5 o clock i live in a house\n",
      "Misspleed Words: ['wakh']\n",
      "Corrected: it go to bed at 10 or clock is wake tv at 5p to clock in live in an house\n",
      "Target: i go to bed at 10 o clock i watch tv at 5 o clock i live in a house\n",
      "-----------------------------\n",
      "Misspelled Sentence: the house is white it has stone up the frount it is the first from bridgebrook and the sexeon from smallerden\n",
      "Misspleed Words: ['frount', 'sexeon']\n",
      "Corrected: the house is white it has stone up the front it is the first from bridgebrook and the season from smallerden\n",
      "Target: the house is white it has stone up the front it is the first from bridgebrook and the second from smallerden\n",
      "-----------------------------\n",
      "Misspelled Sentence: on monday i sometimes go down the farm in the night i wach tv there is bbc and itv\n",
      "Misspleed Words: ['wach']\n",
      "Corrected: on monday it sometimes go down the farm in the night in each tv there is bbc and it\n",
      "Target: on monday i sometimes go down the farm in the night i watch tv there is bbc and itv\n",
      "-----------------------------\n",
      "Misspelled Sentence: we have got anglia like to wach cowboys\n",
      "Misspleed Words: ['wach', 'cowboys']\n",
      "Corrected: we have got anglican like to each cowboy\n",
      "Target: we have got anglia like to watch cowboys\n",
      "-----------------------------\n",
      "Misspelled Sentence: on tuesday i get off the bus and sometimes in the night i go to the youth colbe\n",
      "Misspleed Words: ['sometimes', 'colbe']\n",
      "Corrected: on tuesday it get off the bus and sometimes in the night it go to the youth cole\n",
      "Target: on tuesday i get off the bus and sometimes in the night i go to the youth club\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# print examples of fixes\n",
    "for i in range(10):\n",
    "    print(f\"Misspelled Sentence: {test_df['Misspelled Sentence'][i]}\")\n",
    "    print(f\"Misspleed Words: {test_df['Misspelled Words'][i]}\")\n",
    "    print(f\"Corrected: {updated['Corrected Sentence'][i]}\")\n",
    "    print(f\"Target: {test_df['Original Sentence'][i]}\")\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Summary and Future Directions**\n",
    "\n",
    "In this assignment, we extended Norvig’s classic spelling corrector by incorporating context-sensitive techniques using n-gram language models. We built a more diverse corpus by combining texts from multiple sources (Gutenberg, Reuters, and Brown) and applied preprocessing steps—such as lowercasing, noise reduction, tokenization, and frequency filtering—to improve vocabulary quality. By integrating bigram and trigram models, and then employing an interpolated model with beam search and careful hyperparameter tuning (adjusting alpha and lambda), we achieved a notable increase in accuracy (from 0.21 to 0.36). However, the interpolated model also showed higher perplexity, cross-entropy, WER, and CER, reflecting the challenges of a small corpus and sparse data when using higher-order n-grams.\n",
    "\n",
    "**Future Improvements:**\n",
    "\n",
    "1. **Increasing Data Size:**  \n",
    "   Expanding the corpus with more diverse and larger datasets will help improve probability estimates, reduce perplexity, and better capture the nuances of language.\n",
    "\n",
    "2. **Handling Keyboard Misspellings:**  \n",
    "   Incorporate techniques specifically designed for keyboard errors. For example:\n",
    "   - **Proximity-Based Corrections:** Use the physical layout of keyboards to determine likely mistypes (e.g., substituting letters that are adjacent).\n",
    "   - **Error Modeling:** Develop a model that learns common typing errors, such as transpositions or repeated characters, to better predict intended words.\n",
    "\n",
    "3. **Advanced Data Structures and Storage:**  \n",
    "   Explore more efficient storage mechanisms for n-grams—such as trie data structures, database solutions with indexing, or compressed storage—to manage larger corpora more effectively.\n",
    "\n",
    "4. **Leveraging Advanced Models:**  \n",
    "   Investigate modern approaches like transformer-based models or other deep learning techniques that may further improve context-sensitive corrections.\n",
    "\n",
    "These future directions aim to enhance both the robustness and efficiency of the spelling correction system while addressing the limitations observed with the current dataset and n-gram approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NOTE RELATED TO THE MAIN SOLUTION OF ASSIGNMENT, ONLY FOR ADDITIONAL POINTS OR RESPECT FOR THE CREATIVITY AND RESOURCEFULNESS!!!**\n",
    "\n",
    "### N-gram model based on the Google Books n-gram API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the assignment completing, I try to discover effective tools to:\n",
    "1) Have the diverge and big base of N-gram data\n",
    "2) Have the ability to not compute stats by own and collect train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "class ContextualSpellingCorrector:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = set(vocabulary)\n",
    "\n",
    "    def run_query(self, query, start_year=2010, end_year=2019, corpus=26, smoothing=3):\n",
    "        \"\"\"Fetches frequency data from the Google Books Ngram API.\"\"\"\n",
    "        query = urllib.parse.quote(query)\n",
    "        url = f'https://books.google.com/ngrams/json?content={query}&year_start={start_year}&year_end={end_year}&corpus={corpus}&smoothing={smoothing}'\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        try:\n",
    "            output = response.json()\n",
    "        except:\n",
    "            return {}\n",
    "\n",
    "        if not output:\n",
    "            return {}\n",
    "\n",
    "        return {entry['ngram']: sum(entry['timeseries']) / len(entry['timeseries']) for entry in output}\n",
    "\n",
    "    def average_frequency(self, phrase):\n",
    "        \"\"\"Gets the average frequency of a word or n-gram phrase from Google Ngrams.\"\"\"\n",
    "        data = self.run_query(phrase)\n",
    "        return sum(data.values()) / len(data) if data else 0\n",
    "\n",
    "    def words(self, text):\n",
    "        \"\"\"Tokenizes and lowercases the input text.\"\"\"\n",
    "        return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "    def correction_with_context(self, word, context_window):\n",
    "        \"\"\"Finds the best spelling correction by considering context-based n-gram probabilities.\"\"\"\n",
    "        candidates = self.candidates(word)\n",
    "        \n",
    "        # Generate n-grams with the surrounding context\n",
    "        context_phrases = {candidate: self.form_context_phrases(candidate, context_window) for candidate in candidates}\n",
    "\n",
    "        # Get frequencies for each candidate within its context\n",
    "        context_frequencies = {\n",
    "            candidate: sum(self.average_frequency(phrase) for phrase in phrases)\n",
    "            for candidate, phrases in context_phrases.items()\n",
    "        }\n",
    "\n",
    "        return max(context_frequencies, key=context_frequencies.get)  # Return the best correction\n",
    "\n",
    "    def form_context_phrases(self, candidate, context_window):\n",
    "        \"\"\"Forms bigram and trigram phrases including the candidate.\"\"\"\n",
    "        left_context, right_context = context_window\n",
    "        phrases = []\n",
    "\n",
    "        if left_context:\n",
    "            phrases.append(f\"{left_context} {candidate}\")\n",
    "        if right_context:\n",
    "            phrases.append(f\"{candidate} {right_context}\")\n",
    "        if left_context and right_context:\n",
    "            phrases.append(f\"{left_context} {candidate} {right_context}\")\n",
    "\n",
    "        return phrases\n",
    "\n",
    "    def candidates(self, word):\n",
    "        \"\"\"Generates possible spelling corrections based on known words.\"\"\"\n",
    "        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])\n",
    "\n",
    "    def known(self, words):\n",
    "        \"\"\"Filters words that exist in the vocabulary.\"\"\"\n",
    "        return set(w for w in words if w in self.vocabulary)\n",
    "\n",
    "    def edits1(self, word):\n",
    "        \"\"\"Generates possible single-edit variations of a word.\"\"\"\n",
    "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(self, word):\n",
    "        \"\"\"Generates possible double-edit variations of a word.\"\"\"\n",
    "        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n",
    "\n",
    "# Load corpus and build vocabulary\n",
    "with open(\"data/train/language_corpus.txt\") as f:\n",
    "    train_corpus = f.read()\n",
    "\n",
    "vocabulary = set(re.findall(r'\\w+', train_corpus.lower()))\n",
    "vocabulary.update([\"<START>\", \"<END>\"])  # Add special tokens\n",
    "\n",
    "# Initialize corrector with updated vocabulary\n",
    "corrector = ContextualSpellingCorrector(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected sentence: this is a spelling error\n"
     ]
    }
   ],
   "source": [
    "# Initialize corrector\n",
    "corrector = ContextualSpellingCorrector(vocabulary)\n",
    "\n",
    "# Example usage with context-aware spelling correction\n",
    "sentence = [\"this\", \"is\", \"a\", \"speling\", \"error\"]\n",
    "corrected_list = []\n",
    "\n",
    "for i, word in enumerate(sentence):\n",
    "    left_context = sentence[i - 1] if i > 0 else \"<START>\"\n",
    "    right_context = sentence[i + 1] if i < len(sentence) - 1 else \"<END>\"\n",
    "    \n",
    "    corrected_word = corrector.correction_with_context(word, (left_context, right_context))\n",
    "    corrected_list.append(corrected_word)\n",
    "\n",
    "corrected_sentence = \" \".join(corrected_list)\n",
    "print(f\"Corrected sentence: {corrected_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this solution have pontentia for future improvements, for me it take a lot to get the representative stats, due to the free API limitations (speed and search time). But I am very like to share with you this discovery."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
